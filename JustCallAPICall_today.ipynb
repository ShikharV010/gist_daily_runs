{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNonIJYWFxFq2UGg2X4AvaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShikharV010/gist_daily_runs/blob/main/JustCallAPICall_today.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ6tBmNzzPVF"
      },
      "outputs": [],
      "source": [
        "import requests, json, time, pandas as pd\n",
        "from datetime import datetime\n",
        "from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 1) CONFIG\n",
        "# ────────────────────────────────────────────────────────────\n",
        "API_KEY    = \"cc7718b616f3be5e663be9f132548cbf083fc5e9\"\n",
        "API_SECRET = \"1f26c3c1e9bbf56324f5f9ddb70bab81b42cff38\"\n",
        "\n",
        "DATE_FROM = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "DATE_TO   = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "BASE_URL             = \"https://api.justcall.io/v2.1/calls\"\n",
        "MAX_CALLS_PER_MIN    = 28         # pacing budget\n",
        "MAX_RETRIES          = 8\n",
        "BACKOFF_FACTOR       = 2\n",
        "REQUEST_TIMEOUT      = 20\n",
        "PAGE_GUARD_LIMIT     = 10_000     # absolute safety cap on pages\n",
        "\n",
        "DEFAULT_FLAGS = {\n",
        "    \"fetch_queue_data\": \"false\",\n",
        "    \"fetch_ai_data\":    \"false\",\n",
        "    \"sort\":             \"id\",\n",
        "    \"order\":            \"desc\",\n",
        "    \"per_page\":         100\n",
        "}\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 2) SESSION\n",
        "# ────────────────────────────────────────────────────────────\n",
        "session = requests.Session()\n",
        "session.auth = (API_KEY, API_SECRET)\n",
        "session.headers.update({\"Authorization\": f\"{API_KEY}:{API_SECRET}\"})\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# helpers\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def _pace(last_ts: list[float]):\n",
        "    \"\"\"Simple token bucket: ensure <= MAX_CALLS_PER_MIN per 60s.\"\"\"\n",
        "    if not last_ts:\n",
        "        last_ts.append(time.monotonic())\n",
        "        return\n",
        "    now = time.monotonic()\n",
        "    elapsed = now - last_ts[0]\n",
        "    # Remove timestamps older than 60s\n",
        "    if elapsed >= 60:\n",
        "        last_ts.clear()\n",
        "        last_ts.append(now)\n",
        "        return\n",
        "    if len(last_ts) >= MAX_CALLS_PER_MIN:\n",
        "        sleep_for = 60 - elapsed\n",
        "        if sleep_for > 0:\n",
        "            time.sleep(sleep_for)\n",
        "        last_ts.clear()\n",
        "        last_ts.append(time.monotonic())\n",
        "    else:\n",
        "        last_ts.append(now)\n",
        "\n",
        "def _merge_query(url: str, extra: dict) -> str:\n",
        "    \"\"\"Merge/attach query params to a URL (without duplicating).\"\"\"\n",
        "    u = urlparse(url)\n",
        "    qs = dict(parse_qsl(u.query, keep_blank_values=True))\n",
        "    qs.update(extra)\n",
        "    return urlunparse((u.scheme, u.netloc, u.path, u.params, urlencode(qs, doseq=True), u.fragment))\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 3) Smart GET with retry/429 handling\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def safe_get(url: str, params: dict | None = None) -> dict:\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        r = session.get(url, params=params, timeout=REQUEST_TIMEOUT)\n",
        "        if r.status_code != 429:\n",
        "            r.raise_for_status()\n",
        "            return r.json()\n",
        "        wait = int(r.headers.get(\"Retry-After\", BACKOFF_FACTOR ** attempt))\n",
        "        wait = max(wait, 1)\n",
        "        print(f\"429 → wait {wait}s (retry {attempt+1}/{MAX_RETRIES})\")\n",
        "        time.sleep(wait)\n",
        "    raise RuntimeError(f\"gave up after {MAX_RETRIES} retries → {url}\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 4) Paginated call listing (robust)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def list_calls(date_from: str, date_to: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Robust pagination:\n",
        "      • re-attaches from_datetime/to_datetime + per_page/sort/order on every hop\n",
        "      • detects URL cycles\n",
        "      • dedupes by call_id and stops when a page adds 0 new rows\n",
        "      • respects a hard page guard and basic pacing\n",
        "    \"\"\"\n",
        "    WINDOW_PARAMS = {\"from_datetime\": date_from, \"to_datetime\": date_to}\n",
        "\n",
        "    all_rows: list[dict] = []\n",
        "    seen_urls: set[str] = set()\n",
        "    seen_ids: set[str | int] = set()\n",
        "    last_ts: list[float] = []\n",
        "    page_no = 0\n",
        "    PAGE_GUARD_LIMIT = 10_000  # safety cap\n",
        "\n",
        "    # first URL = base + flags + window\n",
        "    first_params = DEFAULT_FLAGS | WINDOW_PARAMS\n",
        "    url = _merge_query(BASE_URL, first_params)\n",
        "\n",
        "    while url:\n",
        "        page_no += 1\n",
        "        if page_no > PAGE_GUARD_LIMIT:\n",
        "            raise RuntimeError(f\"page guard tripped after {PAGE_GUARD_LIMIT} pages; aborting\")\n",
        "\n",
        "        if url in seen_urls:\n",
        "            print(f\"⚠️  next_page_link repeated (cycle) at page {page_no}; stopping.\")\n",
        "            break\n",
        "        seen_urls.add(url)\n",
        "\n",
        "        _pace(last_ts)\n",
        "\n",
        "        data = safe_get(url, params=None)  # url already has query params\n",
        "        rows = data.get(\"data\", []) or []\n",
        "        ids = [r.get(\"id\") for r in rows if r.get(\"id\") is not None]\n",
        "\n",
        "        # dedupe by id and count how many are actually new\n",
        "        new_rows = [r for r in rows if r.get(\"id\") not in seen_ids]\n",
        "        for i in ids:\n",
        "            if i is not None:\n",
        "                seen_ids.add(i)\n",
        "\n",
        "        min_id = min(ids) if ids else None\n",
        "        max_id = max(ids) if ids else None\n",
        "        print(f\"• page {page_no}: got {len(rows)} calls, {len(new_rows)} new \"\n",
        "              f\"(min_id={min_id}, max_id={max_id})\")\n",
        "\n",
        "        # Extend with only-new rows (prevents runaway duplicates)\n",
        "        all_rows.extend(new_rows)\n",
        "\n",
        "        # Stop conditions\n",
        "        if len(rows) == 0:\n",
        "            print(\"• empty page; done.\")\n",
        "            break\n",
        "        if len(new_rows) == 0:\n",
        "            print(\"• page added 0 new IDs; likely loop/duplicate page; stopping.\")\n",
        "            break\n",
        "\n",
        "        # Next page URL — ensure flags AND window persist on every hop\n",
        "        next_url = data.get(\"next_page_link\")\n",
        "        if not next_url:\n",
        "            print(\"• no next_page_link; done.\")\n",
        "            break\n",
        "\n",
        "        # Re-attach both paging flags and the date window (some APIs drop them)\n",
        "        next_url = _merge_query(next_url, DEFAULT_FLAGS | WINDOW_PARAMS)\n",
        "\n",
        "        if next_url == url:\n",
        "            print(\"⚠️  next_page_link equals current URL; stopping to avoid loop.\")\n",
        "            break\n",
        "\n",
        "        url = next_url\n",
        "\n",
        "    print(f\"✅ fetched total {len(all_rows)} unique calls across {page_no} page(s)\")\n",
        "    return all_rows\n",
        "\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 5) Flatten into DataFrame\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def flatten(details: list[dict]) -> pd.DataFrame:\n",
        "    stamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    rows = []\n",
        "    for d in details:\n",
        "        call_info = d.get(\"call_info\", {}) or {}\n",
        "        rows.append({\n",
        "            \"call_id\":             d.get(\"id\"),\n",
        "            \"call_sid\":            d.get(\"call_sid\"),\n",
        "            \"contact_number\":      d.get(\"contact_number\"),\n",
        "            \"contact_name\":        d.get(\"contact_name\"),\n",
        "            \"contact_email\":       d.get(\"contact_email\"),\n",
        "            \"justcall_number\":     d.get(\"justcall_number\"),\n",
        "            \"justcall_line_name\":  d.get(\"justcall_line_name\"),\n",
        "            \"agent_id\":            d.get(\"agent_id\"),\n",
        "            \"agent_name\":          d.get(\"agent_name\"),\n",
        "            \"agent_email\":         d.get(\"agent_email\"),\n",
        "            \"agent_active\":        d.get(\"agent_active\"),\n",
        "            \"call_date\":           d.get(\"call_date\"),\n",
        "            \"call_time\":           d.get(\"call_time\"),\n",
        "            \"call_user_date\":      d.get(\"call_user_date\"),\n",
        "            \"call_user_time\":      d.get(\"call_user_time\"),\n",
        "            \"cost_incurred\":       d.get(\"cost_incurred\"),\n",
        "            # Nested call_info\n",
        "            \"direction\":               call_info.get(\"direction\"),\n",
        "            \"type\":                    call_info.get(\"type\"),\n",
        "            \"missed_call_reason\":      call_info.get(\"missed_call_reason\"),\n",
        "            \"status\":                  call_info.get(\"status\"),\n",
        "            \"disposition\":             call_info.get(\"disposition\"),\n",
        "            \"notes\":                   call_info.get(\"notes\"),\n",
        "            \"rating\":                  call_info.get(\"rating\"),\n",
        "            \"recording\":               call_info.get(\"recording\"),\n",
        "            \"recording_child\":         call_info.get(\"recording_child\"),\n",
        "            \"voicemail_transcription\": call_info.get(\"voicemail_transcription\"),\n",
        "            \"call_traits\":             json.dumps(call_info.get(\"call_traits\") or []),\n",
        "            \"date_ingested\":           stamp,\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 6) Main workflow\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def run_ingestion():\n",
        "    print(f\"\\n⏳ Fetching calls {DATE_FROM} → {DATE_TO} …\")\n",
        "    rows = list_calls(DATE_FROM, DATE_TO)\n",
        "    print(f\"✓ {len(rows)} calls fetched\")\n",
        "\n",
        "    df = flatten(rows)\n",
        "    print(f\"\\n🏁 finished – {len(df)} rows in final dataframe\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    df_calls = run_ingestion()\n",
        "    # df_calls.to_csv(\"justcall_calls.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlalchemy                       # <- new (needed only if you add dtype=)\n",
        "from sqlalchemy import create_engine, text\n",
        "from datetime import datetime\n",
        "\n",
        "# ───────────── DB config ─────────────\n",
        "engine = create_engine(\n",
        "    \"postgresql://airbyte_user:airbyte_user_password@\"\n",
        "    \"gw-rds-prod.celzx4qnlkfp.us-east-1.rds.amazonaws.com:5432/gw_prod\"\n",
        ")\n",
        "TABLE_SCHEMA = \"gist\"\n",
        "TABLE_NAME   = \"gist_justcallcalldetails\"\n",
        "VIEW_NAME    = \"vw_justcallcalldetails\"\n",
        "\n",
        "# ───────────── DataFrame from ingestion ─────────────\n",
        "df = df_calls.copy()                    # <-- the only change\n",
        "if df.empty:\n",
        "    print(\"🛑 No new data to insert.\"); raise SystemExit\n",
        "\n",
        "df[\"date_ingested\"] = datetime.utcnow().date()   # keep stamp in UTC\n",
        "\n",
        "try:\n",
        "    # 1️⃣  pull existing call_ids (small result set, OK for now)\n",
        "    with engine.connect() as conn:\n",
        "        existing = {row[0] for row in conn.execute(\n",
        "            text(f\"SELECT call_id FROM {TABLE_SCHEMA}.{TABLE_NAME}\")\n",
        "        )}\n",
        "    print(f\"📦 existing rows in DB: {len(existing)}\")\n",
        "\n",
        "    # 2️⃣  filter out duplicates\n",
        "    df_new = df[~df[\"call_id\"].isin(existing)]\n",
        "    print(f\"🆕 rows to insert: {len(df_new)}\")\n",
        "\n",
        "    # 3️⃣  append\n",
        "    if not df_new.empty:\n",
        "        df_new.to_sql(\n",
        "            name=TABLE_NAME,\n",
        "            con=engine,\n",
        "            schema=TABLE_SCHEMA,\n",
        "            if_exists=\"append\",\n",
        "            index=False,\n",
        "            method=\"multi\"\n",
        "            # dtype={\"campaign\": sqlalchemy.dialects.postgresql.JSONB,\n",
        "            #        \"call_info\": sqlalchemy.dialects.postgresql.JSONB}\n",
        "        )\n",
        "        print(\"✅ new rows appended.\")\n",
        "    else:\n",
        "        print(\"🛑 nothing new to append.\")\n",
        "\n",
        "except Exception as e:\n",
        "    # table missing → create from scratch\n",
        "    print(f\"📭 table absent or error querying it → creating afresh.\\n{e}\")\n",
        "    df.to_sql(\n",
        "        name=TABLE_NAME,\n",
        "        con=engine,\n",
        "        schema=TABLE_SCHEMA,\n",
        "        if_exists=\"replace\",\n",
        "        index=False,\n",
        "        method=\"multi\"\n",
        "    )\n",
        "    print(f\"✅ table {TABLE_SCHEMA}.{TABLE_NAME} created.\")\n",
        "\n",
        "# 4️⃣  make / refresh view\n",
        "with engine.begin() as conn:\n",
        "    conn.execute(text(f\"\"\"\n",
        "        CREATE OR REPLACE VIEW {TABLE_SCHEMA}.{VIEW_NAME} AS\n",
        "        SELECT *\n",
        "        FROM   {TABLE_SCHEMA}.{TABLE_NAME};\n",
        "    \"\"\"))\n",
        "print(f\"🪟 view {TABLE_SCHEMA}.{VIEW_NAME} refreshed.\")\n",
        "engine.dispose()\n"
      ],
      "metadata": {
        "id": "jo_VAbWJ34Gj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}