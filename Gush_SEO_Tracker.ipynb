{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqL/8kjwDFggpcpmnIiAo4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShikharV010/gist_daily_runs/blob/main/Gush_SEO_Tracker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll_F-RsLlg9Y"
      },
      "outputs": [],
      "source": [
        "pip install pandas sqlalchemy psycopg2-binary python-dateutil gspread oauth2client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIG ---\n",
        "import os\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "\n",
        "DB_URL = \"postgresql+psycopg2://airbyte_user:airbyte_user_password@gw-rds-prod.celzx4qnlkfp.us-east-1.rds.amazonaws.com:5432/gw_prod\"\n",
        "\n",
        "SCHEMA_GSC  = \"airbyte_ingestion\"\n",
        "TBL_GSC_PQD = \"gush_gsc_page_query_daily\"  # (kept if you still need query-level)\n",
        "TBL_GSC_PD  = \"gush_gsc_page_daily\"        # <-- new: page-level table\n",
        "\n",
        "SCHEMA_DICT = \"airbyte_ingestion\"\n",
        "TABLE_DICT  = \"gtm_seo_gush_seo_pages\"\n",
        "\n",
        "engine = create_engine(DB_URL)\n",
        "\n",
        "# --- READ GSC: page_query_daily (unchanged; optional) ---\n",
        "with engine.connect() as con:\n",
        "    gsc_pqd_sql = text(f\"\"\"\n",
        "        SELECT\n",
        "            date::date          AS date,\n",
        "            page                AS page,\n",
        "            query               AS query,\n",
        "            clicks::bigint      AS clicks,\n",
        "            impressions::bigint AS impressions,\n",
        "            ctr::numeric        AS ctr,\n",
        "            position::numeric   AS position\n",
        "        FROM {SCHEMA_GSC}.{TBL_GSC_PQD}\n",
        "        WHERE date IS NOT NULL\n",
        "    \"\"\")\n",
        "    gsc_page_query_daily_df = pd.read_sql(gsc_pqd_sql, con)\n",
        "\n",
        "# --- READ GSC: page_daily (use this for page-level performance) ---\n",
        "with engine.connect() as con:\n",
        "    gsc_pd_sql = text(f\"\"\"\n",
        "        SELECT\n",
        "            date::date          AS date,\n",
        "            page                AS page,\n",
        "            clicks::bigint      AS clicks,\n",
        "            impressions::bigint AS impressions\n",
        "        FROM {SCHEMA_GSC}.{TBL_GSC_PD}\n",
        "        WHERE date IS NOT NULL\n",
        "    \"\"\")\n",
        "    gsc_page_daily_df = pd.read_sql(gsc_pd_sql, con)\n",
        "\n",
        "# --- READ Pages/Keywords (only the columns you need; exact hero_url strings) ---\n",
        "with engine.connect() as con:\n",
        "    dict_sql = text(f\"\"\"\n",
        "        SELECT\n",
        "            primary_keyword,\n",
        "            secondary_keyword,\n",
        "            hero_url,\n",
        "            volume\n",
        "        FROM {SCHEMA_DICT}.{TABLE_DICT}\n",
        "        WHERE hero_url IS NOT NULL\n",
        "          AND hero_url <> ''\n",
        "          AND hero_url <> 'New'\n",
        "          AND hero_url <> '\\\\'\n",
        "    \"\"\")\n",
        "    dict_raw_df = pd.read_sql(dict_sql, con)\n",
        "\n",
        "#display(gsc_page_daily_df.head(3))\n",
        "#display(dict_raw_df.head(3))\n"
      ],
      "metadata": {
        "id": "zIUA8puulkpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GET QUERY AND PAGE PERFORMANCE"
      ],
      "metadata": {
        "id": "kWKSHB7aUvLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Weekly 28-day SEO performance (daily sums) wide table\n",
        "\n",
        "Joins (EXACT string equality):\n",
        "  - airbyte_ingestion.gush_gsc_page_query_daily  (date, page, query, clicks, impressions)\n",
        "    ×\n",
        "  - airbyte_ingestion.gtm_seo_gush_seo_pages     (primary_keyword, secondary_keyword, hero_url, volume)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# CONFIG\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "DB_URL = os.getenv(\n",
        "    \"DB_URL\",\n",
        "    \"postgresql+psycopg2://airbyte_user:airbyte_user_password@\"\n",
        "    \"gw-rds-prod.celzx4qnlkfp.us-east-1.rds.amazonaws.com:5432/gw_prod\"\n",
        ")\n",
        "\n",
        "SCHEMA_GSC = \"airbyte_ingestion\"\n",
        "TABLE_GSC  = \"gush_gsc_page_query_daily\"\n",
        "\n",
        "SCHEMA_DICT = \"airbyte_ingestion\"\n",
        "TABLE_DICT  = \"gtm_seo_gush_seo_pages\"\n",
        "\n",
        "# Weekly anchors starting from 29 Jun (change if needed)\n",
        "START_ANCHOR_STR = \"2025-06-29\"\n",
        "ANCHOR_FREQ      = \"7D\"\n",
        "\n",
        "OUTPUT_CSV = None  # e.g., \"weekly_28d_sums_wide.csv\"\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Helpers\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "def label_for_anchor(ts: pd.Timestamp) -> str:\n",
        "    return f\"{ts.day}{ts.strftime('%b')}\"\n",
        "\n",
        "def compute_28d_sums(joined_df: pd.DataFrame, anchor: pd.Timestamp) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For a given anchor date, compute 28-day sums of clicks & impressions\n",
        "    for each (primary_keyword, secondary_keyword, hero_url).\n",
        "    \"\"\"\n",
        "    win_start = anchor - pd.Timedelta(days=27)  # inclusive 28D window\n",
        "    jdate = pd.to_datetime(joined_df[\"date\"])\n",
        "    sub = joined_df.loc[\n",
        "        (jdate >= win_start) & (jdate <= anchor),\n",
        "        [\"primary_keyword\",\"secondary_keyword\",\"hero_url\",\"clicks\",\"impressions\"]\n",
        "    ].copy()\n",
        "\n",
        "    label = label_for_anchor(anchor)\n",
        "    if sub.empty:\n",
        "        return pd.DataFrame(columns=[\n",
        "            \"primary_keyword\",\"secondary_keyword\",\"hero_url\",\n",
        "            f\"{label}_clicks\", f\"{label}_impressions\"\n",
        "        ])\n",
        "\n",
        "    agg = (\n",
        "        sub.groupby([\"primary_keyword\",\"secondary_keyword\",\"hero_url\"], as_index=False)\n",
        "           .agg(clicks_28d=(\"clicks\",\"sum\"),\n",
        "                impressions_28d=(\"impressions\",\"sum\"))\n",
        "           .rename(columns={\n",
        "               \"clicks_28d\":      f\"{label}_clicks\",\n",
        "               \"impressions_28d\": f\"{label}_impressions\"\n",
        "           })\n",
        "    )\n",
        "    return agg[[\"primary_keyword\",\"secondary_keyword\",\"hero_url\",\n",
        "                f\"{label}_clicks\", f\"{label}_impressions\"]]\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 1) Read data\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "engine = create_engine(DB_URL)\n",
        "\n",
        "with engine.connect() as con:\n",
        "    gsc_sql = text(f\"\"\"\n",
        "        SELECT\n",
        "            date::date          AS date,\n",
        "            page                AS page,\n",
        "            query               AS query,\n",
        "            clicks::bigint      AS clicks,\n",
        "            impressions::bigint AS impressions\n",
        "        FROM {SCHEMA_GSC}.{TABLE_GSC}\n",
        "        WHERE date IS NOT NULL\n",
        "    \"\"\")\n",
        "    gsc_raw_df = pd.read_sql(gsc_sql, con)\n",
        "\n",
        "with engine.connect() as con:\n",
        "    dict_sql = text(f\"\"\"\n",
        "        SELECT\n",
        "            primary_keyword,\n",
        "            secondary_keyword,\n",
        "            hero_url,\n",
        "            volume\n",
        "        FROM {SCHEMA_DICT}.{TABLE_DICT}\n",
        "        WHERE hero_url IS NOT NULL AND hero_url <> '' AND hero_url <> 'New' AND hero_url <> '\\\\'\n",
        "          AND secondary_keyword IS NOT NULL AND secondary_keyword <> ''\n",
        "    \"\"\")\n",
        "    dict_raw_df = pd.read_sql(dict_sql, con)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 2) Prep: standardize names (values remain unmodified / exact)\n",
        "#    + DEDUP dictionary to one row per (primary, secondary, hero_url)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "gsc = gsc_raw_df.rename(columns=lambda c: c.strip().lower()).copy()\n",
        "kw  = dict_raw_df.rename(columns=lambda c: c.strip().lower()).copy()\n",
        "\n",
        "# Dedup KW to avoid row multiplication on join (keep a stable volume; choose MAX)\n",
        "kw_dedup = (\n",
        "    kw.groupby([\"primary_keyword\",\"secondary_keyword\",\"hero_url\"], as_index=False)\n",
        "      .agg(volume=(\"volume\",\"max\"))\n",
        ")\n",
        "\n",
        "# Optional: ensure unique daily rows in GSC (protect vs source dup granularity)\n",
        "gsc_daily = (\n",
        "    gsc.groupby([\"date\",\"page\",\"query\"], as_index=False)\n",
        "       .agg(clicks=(\"clicks\",\"sum\"), impressions=(\"impressions\",\"sum\"))\n",
        ")\n",
        "\n",
        "# Base keys so even empty weeks keep a row\n",
        "base_keys = kw_dedup[[\"primary_keyword\",\"secondary_keyword\",\"hero_url\",\"volume\"]].copy()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 3) EXACT join: page == hero_url AND query == secondary_keyword\n",
        "#    + re-aggregate after join to collapse any residual dupes\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "joined = gsc_daily.merge(\n",
        "    kw_dedup[[\"primary_keyword\",\"secondary_keyword\",\"hero_url\",\"volume\"]],\n",
        "    left_on=[\"page\",\"query\"],\n",
        "    right_on=[\"hero_url\",\"secondary_keyword\"],\n",
        "    how=\"inner\"\n",
        ")[[\"date\",\"primary_keyword\",\"secondary_keyword\",\"hero_url\",\"volume\",\"clicks\",\"impressions\"]]\n",
        "\n",
        "# Safety: collapse to one row per (date, primary, secondary, hero).\n",
        "joined = (\n",
        "    joined.groupby([\"date\",\"primary_keyword\",\"secondary_keyword\",\"hero_url\",\"volume\"], as_index=False)\n",
        "          .agg(clicks=(\"clicks\",\"sum\"), impressions=(\"impressions\",\"sum\"))\n",
        ")\n",
        "\n",
        "if joined.empty:\n",
        "    raise SystemExit(\"No rows after exact join. Verify hero_url and secondary_keyword match GSC page/query exactly.\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 4) Build weekly anchors (29 Jun → latest available date)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "start_anchor = pd.to_datetime(START_ANCHOR_STR)\n",
        "max_date = pd.to_datetime(joined[\"date\"]).max()\n",
        "anchors = pd.date_range(start=start_anchor, end=max_date, freq=ANCHOR_FREQ)\n",
        "anchors = pd.DatetimeIndex([a for a in anchors if a <= max_date])\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 5) Compute weekly 28D sums and assemble wide\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "wide = base_keys[[\"primary_keyword\",\"secondary_keyword\",\"volume\",\"hero_url\"]].copy()\n",
        "\n",
        "frames = [compute_28d_sums(joined, a) for a in anchors]\n",
        "for dfw in frames:\n",
        "    wide = wide.merge(dfw, on=[\"primary_keyword\",\"secondary_keyword\",\"hero_url\"], how=\"left\")\n",
        "\n",
        "# Fill NaNs with 0 for metric columns, and cast to integers (no decimals)\n",
        "metric_cols = [c for c in wide.columns if c.endswith(\"_clicks\") or c.endswith(\"_impressions\")]\n",
        "if metric_cols:\n",
        "    wide[metric_cols] = wide[metric_cols].fillna(0).astype(\"int64\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 6) Order columns: static first, then weekly pairs in *reverse chronological* order\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "ordered_cols = [\"primary_keyword\",\"secondary_keyword\",\"volume\",\"hero_url\"]\n",
        "for a in anchors[::-1]:  # latest first\n",
        "    lbl = label_for_anchor(a)\n",
        "    ordered_cols += [f\"{lbl}_clicks\", f\"{lbl}_impressions\"]\n",
        "\n",
        "final_wide_df = wide.reindex(columns=ordered_cols)\n",
        "\n",
        "print(f\"Rows: {final_wide_df.shape[0]}, Cols: {final_wide_df.shape[1]}\")\n",
        "print(\"First columns:\", final_wide_df.columns[:8].tolist())\n",
        "print(\"Last columns :\", final_wide_df.columns[-6:].tolist())\n",
        "final_wide_df.head(5)\n",
        "\n",
        "# Optional: save to CSV\n",
        "if OUTPUT_CSV:\n",
        "    final_wide_df.to_csv(OUTPUT_CSV, index=False)\n",
        "    print(f\"Saved: {OUTPUT_CSV}\")\n"
      ],
      "metadata": {
        "id": "GQfMBIFunc1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(final_wide_df)"
      ],
      "metadata": {
        "id": "Y0RT5kZ5pHfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GET RANK"
      ],
      "metadata": {
        "id": "qlxxD7KVSUAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# config\n",
        "SERPER_API_KEY = \"6769b8e78f7e96c5ff1793582bebbe532085d6be\"   # replace with your real key\n",
        "API_URL = \"https://google.serper.dev/search\"\n",
        "\n",
        "headers = {\n",
        "    \"X-API-KEY\": SERPER_API_KEY,\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# run over all keywords\n",
        "df_keywords = dict_raw_df.copy()\n",
        "\n",
        "results = []\n",
        "\n",
        "for idx, row in df_keywords.iterrows():\n",
        "    keyword = str(row[\"secondary_keyword\"]).strip()\n",
        "    hero_url = str(row[\"hero_url\"]).strip()\n",
        "\n",
        "    payload = {\n",
        "        \"q\": keyword,\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\",\n",
        "        \"num\": 100        # fetch up to top 500 results\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        res = requests.post(API_URL, headers=headers, json=payload, timeout=20)\n",
        "        print(f\"{idx}: status {res.status_code}\")   # print status for debugging\n",
        "\n",
        "        res.raise_for_status()\n",
        "        data = res.json()\n",
        "\n",
        "        rank = None\n",
        "        for i, r in enumerate(data.get(\"organic\", []), start=1):\n",
        "            if hero_url in r.get(\"link\", \"\"):\n",
        "                rank = i\n",
        "                break\n",
        "\n",
        "        # assign \"None or >500\" if not found\n",
        "        if rank is None:\n",
        "            rank_value = \"None or >100\"\n",
        "        else:\n",
        "            rank_value = rank\n",
        "\n",
        "        results.append({\n",
        "            \"secondary_keyword\": keyword,\n",
        "            \"hero_url\": hero_url,\n",
        "            \"rank\": rank_value\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error for {keyword}: {e}\")\n",
        "        results.append({\n",
        "            \"secondary_keyword\": keyword,\n",
        "            \"hero_url\": hero_url,\n",
        "            \"rank\": \"None or >500\"\n",
        "        })\n",
        "\n",
        "rank_df = pd.DataFrame(results)\n",
        "print(rank_df)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "n5HxukXa8Z0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MERGE RANK AND WEEK ON WEEK QUERY & PAGE PERFORMANCE"
      ],
      "metadata": {
        "id": "PhgASDd1E_zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Keep only join keys + rank, dedupe\n",
        "rank_clean = (\n",
        "    rank_df[[\"secondary_keyword\", \"hero_url\", \"rank\"]]\n",
        "    .drop_duplicates(subset=[\"secondary_keyword\", \"hero_url\"], keep=\"first\")\n",
        ")\n",
        "\n",
        "# 2) LEFT JOIN\n",
        "merged = final_wide_df.merge(\n",
        "    rank_clean,\n",
        "    on=[\"secondary_keyword\", \"hero_url\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# 3) Reorder so: primary_keyword, secondary_keyword, hero_url, volume, rank, then everything else\n",
        "front = [\"primary_keyword\", \"secondary_keyword\", \"hero_url\", \"volume\", \"rank\"]\n",
        "rest  = [c for c in merged.columns if c not in front]\n",
        "merged = merged[front + rest]\n",
        "\n",
        "merged.head()\n"
      ],
      "metadata": {
        "id": "_YCSawfQFDgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WRITE QUERY AND PAGE PERFORMANCE"
      ],
      "metadata": {
        "id": "gytFj5_vUymJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Write merged to Postgres safely (no DROP) + refresh view ---\n",
        "\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text, inspect\n",
        "\n",
        "# ───────────── DB config ─────────────\n",
        "engine = create_engine(\n",
        "    \"postgresql+psycopg2://airbyte_user:airbyte_user_password@\"\n",
        "    \"gw-rds-prod.celzx4qnlkfp.us-east-1.rds.amazonaws.com:5432/gw_prod\"\n",
        ")\n",
        "\n",
        "TABLE_SCHEMA = \"gist\"\n",
        "TABLE_NAME   = \"gist_gush_query_page_seo\"\n",
        "VIEW_NAME    = \"vw_gist_gush_query_page_seo\"\n",
        "\n",
        "# ───────────── DataFrame to load ─────────────\n",
        "assert 'merged' in globals(), \"merged not found. Run the transform cell first.\"\n",
        "df = merged.copy()\n",
        "if df.empty:\n",
        "    print(\"🛑 merged is empty; nothing to load.\")\n",
        "    engine.dispose()\n",
        "    raise SystemExit\n",
        "\n",
        "# Ensure schema exists\n",
        "with engine.begin() as conn:\n",
        "    conn.execute(text(f'CREATE SCHEMA IF NOT EXISTS \"{TABLE_SCHEMA}\";'))\n",
        "\n",
        "insp = inspect(engine)\n",
        "table_exists = insp.has_table(TABLE_NAME, schema=TABLE_SCHEMA)\n",
        "\n",
        "if not table_exists:\n",
        "    # First run: create table from scratch based on DataFrame\n",
        "    df.to_sql(\n",
        "        name=TABLE_NAME,\n",
        "        con=engine,\n",
        "        schema=TABLE_SCHEMA,\n",
        "        if_exists=\"replace\",  # safe: no dependent view yet\n",
        "        index=False,\n",
        "        method=\"multi\",\n",
        "        chunksize=5_000,\n",
        "    )\n",
        "    print(f\"✅ created {TABLE_SCHEMA}.{TABLE_NAME} with {len(df)} rows\")\n",
        "else:\n",
        "    # Subsequent runs: handle new columns (weekly anchors), then TRUNCATE + APPEND\n",
        "    # 1) Read existing column names (lowercased for comparison)\n",
        "    with engine.connect() as conn:\n",
        "        existing_cols = pd.read_sql(\n",
        "            text(\"\"\"\n",
        "                SELECT column_name\n",
        "                FROM information_schema.columns\n",
        "                WHERE table_schema = :schema AND table_name = :table\n",
        "                ORDER BY ordinal_position\n",
        "            \"\"\"),\n",
        "            conn,\n",
        "            params={\"schema\": TABLE_SCHEMA, \"table\": TABLE_NAME},\n",
        "        )[\"column_name\"].str.lower().tolist()\n",
        "\n",
        "    # 2) Identify missing columns from the new DF\n",
        "    df_cols_lower = [c.lower() for c in df.columns]\n",
        "    missing = [c for c in df.columns if c.lower() not in existing_cols]\n",
        "\n",
        "    # 3) Add any missing columns:\n",
        "    #    - static text columns: primary_keyword, secondary_keyword, hero_url\n",
        "    #    - static numeric column: volume (BIGINT)\n",
        "    #    - metric columns (weekly *_clicks / *_impressions): BIGINT DEFAULT 0\n",
        "    if missing:\n",
        "        text_cols   = {\"primary_keyword\", \"secondary_keyword\", \"hero_url\"}\n",
        "        numeric_cols = {\"volume\"}  # add more static numeric fields here if needed\n",
        "\n",
        "        with engine.begin() as conn:\n",
        "            for col in missing:\n",
        "                col_l = col.lower()\n",
        "                if col_l in (c.lower() for c in text_cols):\n",
        "                    conn.execute(text(\n",
        "                        f'ALTER TABLE \"{TABLE_SCHEMA}\".\"{TABLE_NAME}\" '\n",
        "                        f'ADD COLUMN IF NOT EXISTS \"{col}\" text;'\n",
        "                    ))\n",
        "                elif col_l in (c.lower() for c in numeric_cols):\n",
        "                    conn.execute(text(\n",
        "                        f'ALTER TABLE \"{TABLE_SCHEMA}\".\"{TABLE_NAME}\" '\n",
        "                        f'ADD COLUMN IF NOT EXISTS \"{col}\" bigint;'\n",
        "                    ))\n",
        "                else:\n",
        "                    # Treat all metric columns as BIGINT with default 0\n",
        "                    conn.execute(text(\n",
        "                        f'ALTER TABLE \"{TABLE_SCHEMA}\".\"{TABLE_NAME}\" '\n",
        "                        f'ADD COLUMN IF NOT EXISTS \"{col}\" bigint DEFAULT 0;'\n",
        "                    ))\n",
        "        print(f\"🧩 added {len(missing)} new column(s): {missing}\")\n",
        "\n",
        "    # 4) TRUNCATE target (keeps dependent view intact), then APPEND all rows\n",
        "    with engine.begin() as conn:\n",
        "        conn.execute(text(f'TRUNCATE TABLE \"{TABLE_SCHEMA}\".\"{TABLE_NAME}\";'))\n",
        "\n",
        "    df.to_sql(\n",
        "        name=TABLE_NAME,\n",
        "        con=engine,\n",
        "        schema=TABLE_SCHEMA,\n",
        "        if_exists=\"append\",   # we just truncated; do not replace\n",
        "        index=False,\n",
        "        method=\"multi\",\n",
        "        chunksize=5_000,\n",
        "    )\n",
        "    print(f\"✅ truncated & loaded {len(df)} rows into {TABLE_SCHEMA}.{TABLE_NAME}\")\n",
        "\n",
        "# 5) (Re)create passthrough view (idempotent)\n",
        "with engine.begin() as conn:\n",
        "    conn.execute(text(f'''\n",
        "        CREATE OR REPLACE VIEW \"{TABLE_SCHEMA}\".\"{VIEW_NAME}\" AS\n",
        "        SELECT * FROM \"{TABLE_SCHEMA}\".\"{TABLE_NAME}\";\n",
        "    '''))\n",
        "\n",
        "print(f\"🪟 view {TABLE_SCHEMA}.{VIEW_NAME} refreshed.\")\n",
        "engine.dispose()\n"
      ],
      "metadata": {
        "id": "7mr9F_bZqhsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GET PAGE PERFORMANCE"
      ],
      "metadata": {
        "id": "_G7aGfhFU08Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Exact-URL page performance (28-day sums, weekly anchors)\n",
        "Uses gush_gsc_page_daily for page-level parity with GSC \"Page\" view.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# ---- Params ----\n",
        "START_ANCHOR_STR = \"2025-06-29\"  # first weekly anchor (e.g., Sunday)\n",
        "ANCHOR_FREQ      = \"7D\"          # weekly cadence\n",
        "\n",
        "# ---- Helpers ----\n",
        "def label_for_anchor(ts: pd.Timestamp) -> str:\n",
        "    return f\"{ts.day}{ts.strftime('%b')}\"\n",
        "\n",
        "def compute_28d_sums(pages_join_df: pd.DataFrame, anchor: pd.Timestamp) -> pd.DataFrame:\n",
        "    win_start = anchor - pd.Timedelta(days=27)  # inclusive window\n",
        "    jdate = pd.to_datetime(pages_join_df[\"date\"])\n",
        "    sub = pages_join_df.loc[(jdate >= win_start) & (jdate <= anchor),\n",
        "                            [\"hero_url\", \"clicks\", \"impressions\"]].copy()\n",
        "    label = label_for_anchor(anchor)\n",
        "    if sub.empty:\n",
        "        return pd.DataFrame(columns=[\"hero_url\", f\"{label}_clicks\", f\"{label}_impressions\"])\n",
        "    agg = (sub.groupby(\"hero_url\", as_index=False)\n",
        "             .agg(**{\"%s_clicks\" % label: (\"clicks\",\"sum\"),\n",
        "                     \"%s_impressions\" % label: (\"impressions\",\"sum\")}))\n",
        "    return agg\n",
        "\n",
        "# ---- EXACT join: hero_url == page (no normalization) ----\n",
        "# Dedup pages list\n",
        "base_pages_df = (\n",
        "    dict_raw_df[[\"hero_url\"]]\n",
        "    .dropna()\n",
        "    .drop_duplicates()\n",
        "    .query(\"hero_url != '' and hero_url != 'New' and hero_url != '\\\\\\\\'\")\n",
        "    .copy()\n",
        ")\n",
        "\n",
        "# Aggregate GSC to one row per (date, page) (protects vs duplicates in source)\n",
        "gsc_day = (gsc_page_daily_df\n",
        "           .groupby([\"date\",\"page\"], as_index=False)\n",
        "           .agg(clicks=(\"clicks\",\"sum\"), impressions=(\"impressions\",\"sum\")))\n",
        "\n",
        "# Exact equality join\n",
        "pages_join_df = gsc_day.merge(\n",
        "    base_pages_df.rename(columns={\"hero_url\":\"page\"}),\n",
        "    on=\"page\",\n",
        "    how=\"inner\"\n",
        ").rename(columns={\"page\":\"hero_url\"})\n",
        "\n",
        "if pages_join_df.empty:\n",
        "    raise SystemExit(\"No exact matches between hero_url and GSC.page. Check strings in the sheet vs GSC.\")\n",
        "\n",
        "# ---- Build anchors up to latest weekly anchor ≤ today (UTC) ----\n",
        "start_anchor_d = pd.to_datetime(START_ANCHOR_STR).date()\n",
        "today_utc_d    = pd.Timestamp.utcnow().date()\n",
        "weeks_since_start = max(0, (today_utc_d - start_anchor_d).days // 7)\n",
        "anchor_end = pd.Timestamp(start_anchor_d) + pd.Timedelta(days=7 * weeks_since_start)\n",
        "anchors = pd.date_range(start=pd.Timestamp(start_anchor_d), end=anchor_end, freq=ANCHOR_FREQ)\n",
        "\n",
        "# ---- Compute week-on-week 28D sums → wide ----\n",
        "wide = base_pages_df[[\"hero_url\"]].copy()\n",
        "frames = [compute_28d_sums(pages_join_df, a) for a in anchors]\n",
        "for dfw in frames:\n",
        "    wide = wide.merge(dfw, on=\"hero_url\", how=\"left\")\n",
        "\n",
        "# Fill NaNs with 0 for metric columns, then CAST to integers (no decimals)\n",
        "metric_cols = [c for c in wide.columns if c.endswith(\"_clicks\") or c.endswith(\"_impressions\")]\n",
        "if metric_cols:\n",
        "    wide[metric_cols] = wide[metric_cols].fillna(0).astype(\"int64\")  # <- integer-only output\n",
        "\n",
        "# ---- Order columns: hero_url + weekly pairs in *reverse chronological* order ----\n",
        "ordered_cols = [\"hero_url\"]\n",
        "for a in anchors[::-1]:  # latest first\n",
        "    lbl = label_for_anchor(a)\n",
        "    ordered_cols += [f\"{lbl}_clicks\", f\"{lbl}_impressions\"]\n",
        "\n",
        "final_pages_wide_df = wide.reindex(columns=ordered_cols)\n",
        "\n",
        "print(f\"Rows: {final_pages_wide_df.shape[0]}, Cols: {final_pages_wide_df.shape[1]}\")\n",
        "print(\"Sample columns:\", final_pages_wide_df.columns[:6].tolist())\n",
        "final_pages_wide_df.head(5)\n"
      ],
      "metadata": {
        "id": "-LVljv27U5NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WRITE PAGE PERFORMANCE"
      ],
      "metadata": {
        "id": "24N4KwKjU51-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Write final_pages_wide_df to Postgres safely (no DROP on target) + refresh view ---\n",
        "\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text, inspect\n",
        "\n",
        "# ───────────── DB config ─────────────\n",
        "engine = create_engine(\n",
        "    \"postgresql+psycopg2://airbyte_user:airbyte_user_password@\"\n",
        "    \"gw-rds-prod.celzx4qnlkfp.us-east-1.rds.amazonaws.com:5432/gw_prod\"\n",
        ")\n",
        "\n",
        "TABLE_SCHEMA = \"gist\"\n",
        "TABLE_NAME   = \"gist_gush_page_seo\"\n",
        "VIEW_NAME    = \"vw_gist_gush_page_seo\"\n",
        "\n",
        "# ───────────── DataFrame to load ─────────────\n",
        "assert 'final_pages_wide_df' in globals(), \"final_pages_wide_df not found. Run the transform cell first.\"\n",
        "df = final_pages_wide_df.copy()\n",
        "if df.empty:\n",
        "    print(\"🛑 final_pages_wide_df is empty; nothing to load.\")\n",
        "    engine.dispose()\n",
        "    raise SystemExit\n",
        "\n",
        "# ensure schema exists\n",
        "with engine.begin() as conn:\n",
        "    conn.execute(text(f'CREATE SCHEMA IF NOT EXISTS \"{TABLE_SCHEMA}\";'))\n",
        "\n",
        "insp = inspect(engine)\n",
        "table_exists = insp.has_table(TABLE_NAME, schema=TABLE_SCHEMA)\n",
        "\n",
        "if not table_exists:\n",
        "    # First run: create table from scratch\n",
        "    df.to_sql(\n",
        "        name=TABLE_NAME,\n",
        "        con=engine,\n",
        "        schema=TABLE_SCHEMA,\n",
        "        if_exists=\"replace\",  # safe: no dependent view yet\n",
        "        index=False,\n",
        "        method=\"multi\",\n",
        "        chunksize=5_000,\n",
        "    )\n",
        "    print(f\"✅ created {TABLE_SCHEMA}.{TABLE_NAME} with {len(df)} rows\")\n",
        "else:\n",
        "    # Subsequent runs: handle new weekly columns (schema drift), then TRUNCATE + APPEND\n",
        "    # 1) fetch existing column names\n",
        "    with engine.connect() as conn:\n",
        "        existing_cols = pd.read_sql(\n",
        "            text(\"\"\"\n",
        "                SELECT column_name\n",
        "                FROM information_schema.columns\n",
        "                WHERE table_schema = :schema AND table_name = :table\n",
        "                ORDER BY ordinal_position\n",
        "            \"\"\"),\n",
        "            conn,\n",
        "            params={\"schema\": TABLE_SCHEMA, \"table\": TABLE_NAME},\n",
        "        )[\"column_name\"].str.lower().tolist()\n",
        "\n",
        "    df_cols = [c.lower() for c in df.columns]\n",
        "    missing = [c for c in df.columns if c.lower() not in existing_cols]\n",
        "\n",
        "    # 2) add any missing columns (new weekly anchors) as DOUBLE PRECISION, default 0\n",
        "    #    hero_url stays text; all metric columns are numeric\n",
        "    if missing:\n",
        "        with engine.begin() as conn:\n",
        "            for col in missing:\n",
        "                if col.lower() == \"hero_url\":\n",
        "                    conn.execute(text(f'ALTER TABLE \"{TABLE_SCHEMA}\".\"{TABLE_NAME}\" ADD COLUMN IF NOT EXISTS \"{col}\" text;'))\n",
        "                else:\n",
        "                    conn.execute(text(f'ALTER TABLE \"{TABLE_SCHEMA}\".\"{TABLE_NAME}\" ADD COLUMN IF NOT EXISTS \"{col}\" double precision DEFAULT 0;'))\n",
        "        print(f\"🧩 added {len(missing)} new column(s): {missing}\")\n",
        "\n",
        "    # 3) TRUNCATE target (keeps view intact), then APPEND all rows\n",
        "    with engine.begin() as conn:\n",
        "        conn.execute(text(f'TRUNCATE TABLE \"{TABLE_SCHEMA}\".\"{TABLE_NAME}\";'))\n",
        "\n",
        "    df.to_sql(\n",
        "        name=TABLE_NAME,\n",
        "        con=engine,\n",
        "        schema=TABLE_SCHEMA,\n",
        "        if_exists=\"append\",   # <- do NOT replace; we just truncated\n",
        "        index=False,\n",
        "        method=\"multi\",\n",
        "        chunksize=5_000,\n",
        "    )\n",
        "    print(f\"✅ truncated & loaded {len(df)} rows into {TABLE_SCHEMA}.{TABLE_NAME}\")\n",
        "\n",
        "# (re)create passthrough view (idempotent)\n",
        "with engine.begin() as conn:\n",
        "    conn.execute(text(f'''\n",
        "        CREATE OR REPLACE VIEW \"{TABLE_SCHEMA}\".\"{VIEW_NAME}\" AS\n",
        "        SELECT * FROM \"{TABLE_SCHEMA}\".\"{TABLE_NAME}\";\n",
        "    '''))\n",
        "\n",
        "print(f\"🪟 view {TABLE_SCHEMA}.{VIEW_NAME} refreshed.\")\n",
        "engine.dispose()\n"
      ],
      "metadata": {
        "id": "i5UJO4teU8w5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}