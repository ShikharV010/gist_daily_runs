{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShikharV010/gist_daily_runs/blob/main/Page_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6JvkR2Zs7tj",
        "outputId": "99d35a19-f769-4390-fa95-59f27dddddcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.11/dist-packages (2.0.41)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (3.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.14.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.9.10\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading pymongo-4.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.7.0 pymongo-4.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install psycopg2-binary sqlalchemy pandas\n",
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "import pandas as pd\n",
        "import re\n",
        "from sqlalchemy import create_engine, text\n",
        "import pytz\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "import time\n",
        "import math\n",
        "\n",
        "# MongoDB connection parameters\n",
        "mongo_connection_string = \"mongodb+srv://readonlyPAT:readonlyusermongopassword@serverlessinstance1-pe-0.e4u1z9c.mongodb.net/gw_seo_pat\"\n",
        "\n",
        "# PostgreSQL connection parameters\n",
        "pg_params = {\n",
        "    'host': 'gw-postgres-dev.celzx4qnlkfp.us-east-1.rds.amazonaws.com',\n",
        "    'database': 'gw_prod',\n",
        "    'user': 'airbyte_user',\n",
        "    'password': 'airbyte_user_password',\n",
        "    'port': '5432'\n",
        "}\n",
        "\n",
        "# Function to clean domain names\n",
        "def clean_domain(domain):\n",
        "    if domain is None:\n",
        "        return None\n",
        "\n",
        "    # Remove 'sc-domain:' prefix\n",
        "    if 'sc-domain:' in domain:\n",
        "        domain = domain.replace('sc-domain:', '')\n",
        "\n",
        "    # Remove https://, http://, www., blog., blogs., trailing slashes, etc.\n",
        "    domain = re.sub(r'^https?://', '', domain)\n",
        "    domain = re.sub(r'^www\\.', '', domain)\n",
        "    domain = re.sub(r'^blog\\.', '', domain)\n",
        "    domain = re.sub(r'^blogs\\.', '', domain)\n",
        "    domain = domain.strip('/')\n",
        "\n",
        "    # Remove all TLDs (.com, .org, etc.) and any following content\n",
        "    domain = re.sub(r'\\.[a-zA-Z0-9]+(\\.[a-zA-Z0-9]+)*($|/.*$)', '', domain)\n",
        "\n",
        "    # Remove any remaining periods\n",
        "    domain = domain.replace('.', '')\n",
        "\n",
        "    return domain\n",
        "\n",
        "# Function to normalize URLs for better joining\n",
        "def normalize_url(url):\n",
        "    if url is None:\n",
        "        return None\n",
        "\n",
        "    # Convert to lowercase\n",
        "    url = url.lower()\n",
        "\n",
        "    # Remove protocols, www, and common prefixes\n",
        "    url = re.sub(r'^https?://', '', url)\n",
        "    url = re.sub(r'^www\\.', '', url)\n",
        "    url = re.sub(r'^blog\\.', '', url)\n",
        "    url = re.sub(r'^blogs\\.', '', url)\n",
        "\n",
        "    # Remove query parameters\n",
        "    url = re.sub(r'\\?.*$', '', url)\n",
        "\n",
        "    # Remove fragments (anchors)\n",
        "    url = re.sub(r'#.*$', '', url)\n",
        "\n",
        "    # Remove trailing slashes\n",
        "    url = re.sub(r'/+$', '', url)\n",
        "\n",
        "    # Remove trailing index.html, index.php, etc.\n",
        "    url = re.sub(r'/(index)\\.(html|htm|php|asp|aspx)$', '', url)\n",
        "\n",
        "    # Handle common trailing path segments that denote the same content\n",
        "    url = re.sub(r'/(default)\\.(html|htm|php|asp|aspx)$', '', url)\n",
        "\n",
        "    # Replace multiple consecutive slashes with a single slash\n",
        "    url = re.sub(r'/+', '/', url)\n",
        "\n",
        "    # Remove whitespace\n",
        "    url = url.strip()\n",
        "\n",
        "    return url\n",
        "\n",
        "# PostgreSQL optimization - create engine with pooling and timeout settings\n",
        "def create_optimized_engine(pg_params):\n",
        "    pg_conn_string = f\"postgresql://{pg_params['user']}:{pg_params['password']}@{pg_params['host']}:{pg_params['port']}/{pg_params['database']}\"\n",
        "    return create_engine(\n",
        "        pg_conn_string,\n",
        "        pool_size=10,              # Increased number of connections for larger batches\n",
        "        max_overflow=20,           # Allow more additional connections\n",
        "        pool_timeout=60,           # Longer wait for a connection\n",
        "        pool_recycle=1800,         # Recycle connections after 30 minutes\n",
        "        connect_args={\"options\": \"-c statement_timeout=600000\"}  # 10 minute default timeout\n",
        "    )\n",
        "\n",
        "# Create schema and table structure separately (avoid timeouts) - WITH TRUNCATE OPTION\n",
        "def setup_database_schema(engine):\n",
        "    schema_created = False\n",
        "    table_created = False\n",
        "\n",
        "    with engine.connect() as connection:\n",
        "        # Enable autocommit to ensure schema creation is persisted even if later operations fail\n",
        "        connection.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "\n",
        "        try:\n",
        "            # Try setting a very long timeout for schema operations\n",
        "            connection.execute(text(\"SET statement_timeout = 1200000;\"))  # 20 minutes\n",
        "\n",
        "            # Create schema if not exists\n",
        "            connection.execute(text(\"CREATE SCHEMA IF NOT EXISTS gist;\"))\n",
        "            schema_created = True\n",
        "            print(\"Ensured schema 'gist' exists\")\n",
        "\n",
        "            # Check if table exists\n",
        "            table_exists_query = \"\"\"\n",
        "            SELECT EXISTS (\n",
        "                SELECT 1 FROM information_schema.tables\n",
        "                WHERE table_schema = 'gist' AND table_name = 'gist_pageperformance'\n",
        "            );\n",
        "            \"\"\"\n",
        "            table_exists = connection.execute(text(table_exists_query)).scalar()\n",
        "\n",
        "            if not table_exists:\n",
        "                # Create table without indexes first for speed\n",
        "                create_table_sql = \"\"\"\n",
        "                CREATE TABLE gist.gist_pageperformance (\n",
        "                    composite_key TEXT PRIMARY KEY,\n",
        "                    cluster_id TEXT,\n",
        "                    writer TEXT,\n",
        "                    page TEXT,\n",
        "                    domain TEXT,\n",
        "                    cleaned_domain TEXT,\n",
        "                    normalized_page TEXT,\n",
        "                    normalized_post_link TEXT,\n",
        "                    start_date TIMESTAMP WITH TIME ZONE,\n",
        "                    end_date TIMESTAMP WITH TIME ZONE,\n",
        "                    clicks INTEGER,\n",
        "                    impressions INTEGER,\n",
        "                    ctr FLOAT,\n",
        "                    position FLOAT,\n",
        "                    post_link TEXT,\n",
        "                    delivery_date TIMESTAMP WITH TIME ZONE,\n",
        "                    process_run_id TEXT,\n",
        "                    processed_at TIMESTAMP WITH TIME ZONE\n",
        "                );\n",
        "                \"\"\"\n",
        "                connection.execute(text(create_table_sql))\n",
        "                table_created = True\n",
        "                print(\"Created table without indexes\")\n",
        "            else:\n",
        "                print(\"Table already exists - truncating it for fresh data\")\n",
        "\n",
        "                # First drop any views that might depend on this table\n",
        "                try:\n",
        "                    connection.execute(text(\"DROP VIEW IF EXISTS gist.writer_performance_view;\"))\n",
        "                    connection.execute(text(\"DROP VIEW IF EXISTS gist.domain_performance_view;\"))\n",
        "                    print(\"Dropped dependent views\")\n",
        "                except Exception as view_error:\n",
        "                    print(f\"Warning: Could not drop dependent views: {view_error}\")\n",
        "                    print(\"Proceeding with truncate anyway...\")\n",
        "\n",
        "                # Truncate the table (deletes all rows but keeps table structure and indexes)\n",
        "                truncate_sql = \"TRUNCATE TABLE gist.gist_pageperformance;\"\n",
        "                connection.execute(text(truncate_sql))\n",
        "                print(\"Successfully truncated the table\")\n",
        "                table_created = True  # Consider table ready\n",
        "\n",
        "            # Return true in both cases: new table or truncated existing table\n",
        "            return schema_created and table_created\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in schema/table setup: {e}\")\n",
        "            # Return partial success status\n",
        "            return schema_created and table_created\n",
        "\n",
        "# Create indexes separately to avoid timeout\n",
        "def create_table_indexes(engine):\n",
        "    with engine.connect() as connection:\n",
        "        # Enable autocommit for index operations\n",
        "        connection.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "\n",
        "        try:\n",
        "            # Set long timeout for index creation\n",
        "            connection.execute(text(\"SET statement_timeout = 600000;\"))  # 10 minutes\n",
        "\n",
        "            # Check if table exists\n",
        "            table_exists_query = \"\"\"\n",
        "            SELECT EXISTS (\n",
        "                SELECT 1 FROM information_schema.tables\n",
        "                WHERE table_schema = 'gist' AND table_name = 'gist_pageperformance'\n",
        "            );\n",
        "            \"\"\"\n",
        "            table_exists = connection.execute(text(table_exists_query)).scalar()\n",
        "\n",
        "            if not table_exists:\n",
        "                print(\"Table doesn't exist, cannot create indexes\")\n",
        "                return False\n",
        "\n",
        "            # Create each index separately with individual transactions\n",
        "            index_statements = [\n",
        "                \"\"\"CREATE INDEX IF NOT EXISTS idx_pageperformance_page\n",
        "                   ON gist.gist_pageperformance(page);\"\"\",\n",
        "                \"\"\"CREATE INDEX IF NOT EXISTS idx_pageperformance_domain\n",
        "                   ON gist.gist_pageperformance(cleaned_domain);\"\"\",\n",
        "                \"\"\"CREATE INDEX IF NOT EXISTS idx_pageperformance_start_date\n",
        "                   ON gist.gist_pageperformance(start_date);\"\"\",\n",
        "                \"\"\"CREATE INDEX IF NOT EXISTS idx_pageperformance_normalized_page\n",
        "                   ON gist.gist_pageperformance(normalized_page);\"\"\",\n",
        "                \"\"\"CREATE INDEX IF NOT EXISTS idx_pageperformance_normalized_post_link\n",
        "                   ON gist.gist_pageperformance(normalized_post_link);\"\"\"\n",
        "            ]\n",
        "\n",
        "            success_count = 0\n",
        "            for i, stmt in enumerate(index_statements):\n",
        "                try:\n",
        "                    # Create each index in a separate transaction\n",
        "                    with engine.connect() as idx_conn:\n",
        "                        idx_conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "                        idx_conn.execute(text(\"SET statement_timeout = 300000;\"))  # 5 minutes per index\n",
        "                        idx_conn.execute(text(stmt))\n",
        "                    success_count += 1\n",
        "                    print(f\"Created index {i+1}/{len(index_statements)}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not create index {i+1}: {e}\")\n",
        "                    print(\"Will continue without this index\")\n",
        "\n",
        "            print(f\"Created {success_count}/{len(index_statements)} indexes\")\n",
        "            return success_count > 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating indexes: {e}\")\n",
        "            return False\n",
        "\n",
        "# Direct batch insertion with larger batch size (1000 rows)\n",
        "def insert_data_in_batches(engine, dataframe, batch_size=50000):\n",
        "    if dataframe.empty:\n",
        "        print(\"No data to insert\")\n",
        "        return 0\n",
        "\n",
        "    # Remove duplicates before insertion to avoid primary key violations\n",
        "    print(\"Checking for duplicate composite keys in the dataset...\")\n",
        "    duplicates = dataframe.duplicated(subset=['composite_key'], keep='first')\n",
        "    if duplicates.any():\n",
        "        dup_count = duplicates.sum()\n",
        "        print(f\"Found {dup_count} duplicate composite keys. Keeping only the first occurrence of each.\")\n",
        "        # Keep only the first occurrence of each composite_key\n",
        "        dataframe = dataframe.drop_duplicates(subset=['composite_key'], keep='first')\n",
        "\n",
        "    total_rows = len(dataframe)\n",
        "    total_batches = math.ceil(total_rows / batch_size)\n",
        "    rows_inserted = 0\n",
        "\n",
        "    print(f\"Inserting {total_rows} rows in {total_batches} batches (size {batch_size})\")\n",
        "\n",
        "    for i in range(0, total_rows, batch_size):\n",
        "        batch_num = i // batch_size + 1\n",
        "        end_idx = min(i + batch_size, total_rows)\n",
        "        batch_df = dataframe.iloc[i:end_idx].copy()\n",
        "\n",
        "        print(f\"Processing batch {batch_num}/{total_batches} ({len(batch_df)} records)\")\n",
        "\n",
        "        try:\n",
        "            with engine.connect() as conn:\n",
        "                # Enable autocommit for this connection\n",
        "                conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "\n",
        "                # Set reasonable timeout\n",
        "                conn.execute(text(\"SET statement_timeout = 600000;\"))  # 10 minutes for larger batches\n",
        "\n",
        "                # Handle NULL values\n",
        "                batch_df = batch_df.where(pd.notnull(batch_df), None)\n",
        "\n",
        "                # Use to_sql with method='multi' for better performance\n",
        "                batch_df.to_sql(\n",
        "                    'gist_pageperformance',\n",
        "                    conn,\n",
        "                    schema='gist',\n",
        "                    if_exists='append',\n",
        "                    index=False,\n",
        "                    method='multi',\n",
        "                    chunksize=100  # Smaller internal chunks\n",
        "                )\n",
        "\n",
        "                rows_inserted += len(batch_df)\n",
        "                print(f\"Batch {batch_num} processed, total progress: {rows_inserted}/{total_rows} rows\")\n",
        "\n",
        "        except Exception as batch_error:\n",
        "            print(f\"Error processing batch {batch_num}: {str(batch_error)[:200]}...\")\n",
        "            print(\"Falling back to smaller batches for this chunk\")\n",
        "\n",
        "            # Try smaller batches as fallback\n",
        "            small_batch_size = 50\n",
        "            small_total_batches = math.ceil(len(batch_df) / small_batch_size)\n",
        "\n",
        "            for j in range(0, len(batch_df), small_batch_size):\n",
        "                small_batch_num = j // small_batch_size + 1\n",
        "                small_end_idx = min(j + small_batch_size, len(batch_df))\n",
        "                small_batch_df = batch_df.iloc[j:small_end_idx].copy()\n",
        "\n",
        "                print(f\"Processing small batch {small_batch_num}/{small_total_batches}\")\n",
        "\n",
        "                try:\n",
        "                    with engine.connect() as small_conn:\n",
        "                        small_conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "                        small_conn.execute(text(\"SET statement_timeout = 300000;\"))  # 5 minutes\n",
        "\n",
        "                        small_batch_df = small_batch_df.where(pd.notnull(small_batch_df), None)\n",
        "\n",
        "                        small_batch_df.to_sql(\n",
        "                            'gist_pageperformance',\n",
        "                            small_conn,\n",
        "                            schema='gist',\n",
        "                            if_exists='append',\n",
        "                            index=False,\n",
        "                            method='multi',\n",
        "                            chunksize=10\n",
        "                        )\n",
        "\n",
        "                        rows_inserted += len(small_batch_df)\n",
        "                        print(f\"Small batch processed, total progress: {rows_inserted}/{total_rows} rows\")\n",
        "\n",
        "                except Exception as small_batch_error:\n",
        "                    print(f\"Error processing small batch: {str(small_batch_error)[:200]}...\")\n",
        "                    print(\"Trying row-by-row for this batch\")\n",
        "\n",
        "                    # Try individual row insertion as final fallback\n",
        "                    for idx, row in small_batch_df.iterrows():\n",
        "                        try:\n",
        "                            # Handle NaN values\n",
        "                            row = row.where(pd.notnull(row), None)\n",
        "\n",
        "                            # Convert to DataFrame with single row\n",
        "                            single_row_df = pd.DataFrame([row.to_dict()])\n",
        "\n",
        "                            with engine.connect() as single_conn:\n",
        "                                single_conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "                                single_row_df.to_sql(\n",
        "                                    'gist_pageperformance',\n",
        "                                    single_conn,\n",
        "                                    schema='gist',\n",
        "                                    if_exists='append',\n",
        "                                    index=False\n",
        "                                )\n",
        "                                rows_inserted += 1\n",
        "\n",
        "                        except Exception as row_error:\n",
        "                            if \"duplicate key\" in str(row_error).lower():\n",
        "                                print(f\"Skipping duplicate key for row {idx}\")\n",
        "                            else:\n",
        "                                print(f\"Error inserting row {idx}: {str(row_error)[:100]}...\")\n",
        "\n",
        "        # Small pause between batches\n",
        "        time.sleep(0.3)\n",
        "\n",
        "    # Add an explicit final commit to ensure all data is committed\n",
        "    try:\n",
        "        with engine.connect() as final_conn:\n",
        "            final_conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "            # Execute a dummy query to ensure connection is good\n",
        "            final_conn.execute(text(\"SELECT 1;\"))\n",
        "            print(\"Final connection verified and committed\")\n",
        "    except Exception as e:\n",
        "        print(f\"Final commit attempt error: {e}\")\n",
        "\n",
        "    return rows_inserted\n",
        "\n",
        "# Step 4: Validate insertions and run consistency checks\n",
        "def validate_data(engine, original_df):\n",
        "    print(\"\\n===== STEP 4: Validating inserted data =====\")\n",
        "    try:\n",
        "        with engine.connect() as conn:\n",
        "            # Enable autocommit\n",
        "            conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "\n",
        "            # Set timeout\n",
        "            conn.execute(text(\"SET statement_timeout = 300000;\"))  # 5 minutes\n",
        "\n",
        "            # Check if the table exists\n",
        "            table_exists_query = \"\"\"\n",
        "            SELECT EXISTS (\n",
        "                SELECT 1 FROM information_schema.tables\n",
        "                WHERE table_schema = 'gist' AND table_name = 'gist_pageperformance'\n",
        "            );\n",
        "            \"\"\"\n",
        "            table_exists = conn.execute(text(table_exists_query)).scalar()\n",
        "\n",
        "            if not table_exists:\n",
        "                print(\"VALIDATION FAILED: Table does not exist!\")\n",
        "                return False\n",
        "\n",
        "            # Count records\n",
        "            count_query = \"SELECT COUNT(*) FROM gist.gist_pageperformance;\"\n",
        "            db_count = conn.execute(text(count_query)).scalar()\n",
        "\n",
        "            # Compare with expected count\n",
        "            expected_count = len(original_df.drop_duplicates(subset=['composite_key'], keep='first'))\n",
        "\n",
        "            print(f\"Records in database: {db_count}\")\n",
        "            print(f\"Expected records: {expected_count}\")\n",
        "\n",
        "            if db_count == 0:\n",
        "                print(\"VALIDATION FAILED: No records in database!\")\n",
        "                return False\n",
        "\n",
        "            # Sample some records to verify content\n",
        "            sample_query = \"\"\"\n",
        "            SELECT composite_key, cluster_id, page, post_link\n",
        "            FROM gist.gist_pageperformance\n",
        "            LIMIT 5;\n",
        "            \"\"\"\n",
        "\n",
        "            samples = conn.execute(text(sample_query)).fetchall()\n",
        "            if samples:\n",
        "                print(\"Sample records from database:\")\n",
        "                for sample in samples:\n",
        "                    print(f\"  - {sample}\")\n",
        "\n",
        "            # Check for data distribution\n",
        "            stats_query = \"\"\"\n",
        "            SELECT\n",
        "                COUNT(*) as total_records,\n",
        "                COUNT(DISTINCT writer) as distinct_writers,\n",
        "                COUNT(DISTINCT cleaned_domain) as distinct_domains,\n",
        "                MIN(start_date) as earliest_date,\n",
        "                MAX(start_date) as latest_date\n",
        "            FROM gist.gist_pageperformance;\n",
        "            \"\"\"\n",
        "\n",
        "            stats = conn.execute(text(stats_query)).fetchone()\n",
        "            if stats:\n",
        "                print(\"\\nData statistics:\")\n",
        "                print(f\"Total records: {stats[0]}\")\n",
        "                print(f\"Distinct writers: {stats[1]}\")\n",
        "                print(f\"Distinct domains: {stats[2]}\")\n",
        "                print(f\"Date range: {stats[3]} to {stats[4]}\")\n",
        "\n",
        "            # Additional count verification\n",
        "            try:\n",
        "                # Count with different methods\n",
        "                count1 = conn.execute(text(\"SELECT COUNT(*) FROM gist.gist_pageperformance;\")).scalar()\n",
        "                count2 = conn.execute(text(\"SELECT COUNT(1) FROM gist.gist_pageperformance;\")).scalar()\n",
        "\n",
        "                # Count by specific columns\n",
        "                count_by_key = conn.execute(text(\"SELECT COUNT(DISTINCT composite_key) FROM gist.gist_pageperformance;\")).scalar()\n",
        "                count_by_page = conn.execute(text(\"SELECT COUNT(DISTINCT page) FROM gist.gist_pageperformance;\")).scalar()\n",
        "\n",
        "                print(\"\\nDetailed count verification:\")\n",
        "                print(f\"COUNT(*): {count1}\")\n",
        "                print(f\"COUNT(1): {count2}\")\n",
        "                print(f\"COUNT(DISTINCT composite_key): {count_by_key}\")\n",
        "                print(f\"COUNT(DISTINCT page): {count_by_page}\")\n",
        "\n",
        "                # Check if all records from the current run are present\n",
        "                current_run_count = conn.execute(text(f\"SELECT COUNT(*) FROM gist.gist_pageperformance WHERE process_run_id = '{run_id}';\")).scalar()\n",
        "                print(f\"Records from current run (process_run_id = {run_id}): {current_run_count}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Additional verification failed: {e}\")\n",
        "\n",
        "            return db_count > 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Validation failed with error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Main script execution starts here\n",
        "print(\"Connecting to PostgreSQL...\")\n",
        "engine = create_optimized_engine(pg_params)\n",
        "\n",
        "# Query for clusters data\n",
        "clusters_query = \"\"\"\n",
        "SELECT id, writer, post_link, post_date as delivery_date\n",
        "FROM public.clusters\n",
        "WHERE cluster_status = 'posted'\n",
        "AND post_date > CURRENT_DATE - INTERVAL '7 months'\n",
        "\"\"\"\n",
        "\n",
        "pg_clusters_df = pd.read_sql(clusters_query, engine)\n",
        "print(f\"Fetched {len(pg_clusters_df)} clusters from PostgreSQL\")\n",
        "\n",
        "# Ensure dates are timezone-aware\n",
        "if 'delivery_date' in pg_clusters_df.columns:\n",
        "    pg_clusters_df['delivery_date'] = pd.to_datetime(pg_clusters_df['delivery_date'])\n",
        "    if pg_clusters_df['delivery_date'].dt.tz is None:\n",
        "        pg_clusters_df['delivery_date'] = pg_clusters_df['delivery_date'].dt.tz_localize('UTC')\n",
        "    else:\n",
        "        pg_clusters_df['delivery_date'] = pg_clusters_df['delivery_date'].dt.tz_convert('UTC')\n",
        "\n",
        "# Apply normalization to post_link\n",
        "print(\"Normalizing post_link values...\")\n",
        "pg_clusters_df['normalized_post_link'] = pg_clusters_df['post_link'].apply(normalize_url)\n",
        "\n",
        "# Connect to MongoDB\n",
        "print(\"Connecting to MongoDB...\")\n",
        "mongo_client = MongoClient(mongo_connection_string)\n",
        "db_name = \"gw_seo_pat\"\n",
        "mongo_db = mongo_client[db_name]\n",
        "collection = mongo_db[\"page_performance\"]\n",
        "\n",
        "# Calculate date 6 months ago\n",
        "four_months_ago = datetime.now() - timedelta(days=7*30)  # Approximate 7 months\n",
        "four_months_ago = four_months_ago.replace(tzinfo=pytz.UTC)  # Add UTC timezone\n",
        "\n",
        "# Query MongoDB with filter for startDate within last 7 months\n",
        "query = {\"startDate\": {\"$gte\": four_months_ago}}\n",
        "print(f\"Fetching MongoDB data with startDate >= {four_months_ago.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# Get filtered documents from MongoDB\n",
        "all_docs = list(collection.find(query))\n",
        "for doc in all_docs:\n",
        "    if '_id' in doc:\n",
        "        doc['_id'] = str(doc['_id'])\n",
        "    if 'domain' in doc:\n",
        "        doc['cleaned_domain'] = clean_domain(doc['domain'])\n",
        "\n",
        "# Convert to DataFrame\n",
        "mongo_df = pd.DataFrame(all_docs)\n",
        "print(f\"Fetched {len(mongo_df)} records from MongoDB (filtered for last 4 months)\")\n",
        "\n",
        "# Apply normalization to page URLs\n",
        "print(\"Normalizing page URLs...\")\n",
        "mongo_df['normalized_page'] = mongo_df['page'].apply(normalize_url)\n",
        "\n",
        "# Convert MongoDB date fields to pandas datetime with UTC timezone\n",
        "if 'startDate' in mongo_df.columns:\n",
        "    mongo_df['startDate'] = pd.to_datetime(mongo_df['startDate']).dt.tz_localize('UTC')\n",
        "if 'endDate' in mongo_df.columns:\n",
        "    mongo_df['endDate'] = pd.to_datetime(mongo_df['endDate']).dt.tz_localize('UTC')\n",
        "\n",
        "# Join MongoDB data with clusters based on normalized URLs\n",
        "print(\"Joining MongoDB data with clusters using normalized URLs...\")\n",
        "# Join MongoDB and clusters with an inner join on normalized fields\n",
        "final_df = pd.merge(\n",
        "    mongo_df,\n",
        "    pg_clusters_df,\n",
        "    left_on='normalized_page',\n",
        "    right_on='normalized_post_link',\n",
        "    how='inner'\n",
        ")\n",
        "print(f\"After joining MongoDB with clusters: {len(final_df)} records\")\n",
        "print(f\"Records lost in this join: {len(mongo_df) - len(final_df)}\")\n",
        "\n",
        "# Sample some new matches to validate normalization\n",
        "if len(final_df) > 0:\n",
        "    new_matches = final_df[final_df['page'] != final_df['post_link']]\n",
        "    if not new_matches.empty:\n",
        "        print(f\"\\nNew matches found through normalization: {len(new_matches)}\")\n",
        "        print(\"Sample of new matches (showing original URLs):\")\n",
        "        print(new_matches[['page', 'post_link']].head(5).to_string())\n",
        "\n",
        "# Generate run ID and timestamp for tracking\n",
        "run_id = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "current_timestamp = datetime.now()\n",
        "\n",
        "# Add processing metadata\n",
        "if not final_df.empty:\n",
        "    final_df['process_run_id'] = run_id\n",
        "    final_df['processed_at'] = current_timestamp\n",
        "\n",
        "    # Create a composite key for identifying records\n",
        "    final_df['composite_key'] = final_df.apply(\n",
        "        lambda row: f\"{row.get('id', '')}_{row.get('page', '')}_{row.get('startDate', '').strftime('%Y%m%d')}\",\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: Check for and handle duplicate composite keys before proceeding\n",
        "    duplicate_keys = final_df.duplicated(subset=['composite_key'], keep=False)\n",
        "    if duplicate_keys.any():\n",
        "        print(f\"\\nWARNING: Found {duplicate_keys.sum()} rows with duplicate composite keys\")\n",
        "        # Keep only the first occurrence of each composite_key\n",
        "        print(\"Removing duplicate composite keys (keeping first occurrence only)\")\n",
        "        final_df = final_df.drop_duplicates(subset=['composite_key'], keep='first')\n",
        "        print(f\"Dataset after removing duplicates: {len(final_df)} records\")\n",
        "\n",
        "    print(f\"Final dataset prepared with {len(final_df)} records\")\n",
        "\n",
        "    # Column mapping for database\n",
        "    column_mapping = {\n",
        "        'composite_key': 'composite_key',\n",
        "        'id': 'cluster_id',\n",
        "        'writer': 'writer',\n",
        "        'page': 'page',\n",
        "        'domain': 'domain',\n",
        "        'cleaned_domain': 'cleaned_domain',\n",
        "        'normalized_page': 'normalized_page',\n",
        "        'normalized_post_link': 'normalized_post_link',\n",
        "        'startDate': 'start_date',\n",
        "        'endDate': 'end_date',\n",
        "        'clicks': 'clicks',\n",
        "        'impressions': 'impressions',\n",
        "        'ctr': 'ctr',\n",
        "        'position': 'position',\n",
        "        'post_link': 'post_link',\n",
        "        'delivery_date': 'delivery_date',\n",
        "        'process_run_id': 'process_run_id',\n",
        "        'processed_at': 'processed_at'\n",
        "    }\n",
        "\n",
        "    # Only keep the columns we want\n",
        "    columns_to_keep = list(column_mapping.keys())\n",
        "    columns_to_keep = [col for col in columns_to_keep if col in final_df.columns]\n",
        "    subset_df = final_df[columns_to_keep].copy()\n",
        "\n",
        "    # Rename columns according to the mapping\n",
        "    renamed_df = subset_df.rename(columns={k: v for k, v in column_mapping.items() if k in subset_df.columns})\n",
        "\n",
        "    # Save original dataframe for validation\n",
        "    original_df = renamed_df.copy()\n",
        "\n",
        "    # STEP 1: Setup schema and table structure (separate step)\n",
        "    print(\"\\n===== STEP 1: Setting up database schema =====\")\n",
        "    setup_success = setup_database_schema(engine)\n",
        "\n",
        "    if setup_success:\n",
        "        # STEP 2: Create database indexes (separate step)\n",
        "        print(\"\\n===== STEP 2: Creating table indexes =====\")\n",
        "        index_success = create_table_indexes(engine)\n",
        "\n",
        "        # STEP 3: Insert data using direct batch insertion with duplicate handling\n",
        "        print(\"\\n===== STEP 3: Inserting data =====\")\n",
        "        # Use larger batch size (2000 rows)\n",
        "        rows_inserted = insert_data_in_batches(engine, renamed_df, batch_size=2000)\n",
        "\n",
        "        # STEP 4: Validate data insertion\n",
        "        validation_success = validate_data(engine, original_df)\n",
        "\n",
        "        # No dependent views creation - removed as requested\n",
        "    else:\n",
        "        print(\"Failed to set up database schema\")\n",
        "\n",
        "else:\n",
        "    print(\"No data to insert into PostgreSQL\")\n",
        "\n",
        "# Final verification\n",
        "print(\"\\n===== FINAL VERIFICATION =====\")\n",
        "try:\n",
        "    with engine.connect() as final_verify:\n",
        "        final_verify.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "        final_count = final_verify.execute(text(\"SELECT COUNT(*) FROM gist.gist_pageperformance;\")).scalar()\n",
        "        print(f\"Final count in database: {final_count}\")\n",
        "        print(f\"Expected count: {len(final_df.drop_duplicates(subset=['composite_key'], keep='first'))}\")\n",
        "\n",
        "        # Check for this run's records\n",
        "        run_count = final_verify.execute(text(f\"SELECT COUNT(*) FROM gist.gist_pageperformance WHERE process_run_id = '{run_id}';\")).scalar()\n",
        "        print(f\"Records with current run_id ({run_id}): {run_count}\")\n",
        "\n",
        "        # Check if there are older records\n",
        "        other_runs = final_verify.execute(text(f\"SELECT process_run_id, COUNT(*) FROM gist.gist_pageperformance WHERE process_run_id != '{run_id}' GROUP BY process_run_id;\")).fetchall()\n",
        "        if other_runs:\n",
        "            print(\"Found records from other runs:\")\n",
        "            for other_run in other_runs:\n",
        "                print(f\"  - Run ID {other_run[0]}: {other_run[1]} records\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Final verification failed: {e}\")\n",
        "\n",
        "# Output summary\n",
        "print(\"\\n===== SUMMARY =====\")\n",
        "print(f\"MongoDB records (filtered for last 4 months): {len(mongo_df)}\")\n",
        "print(f\"Clusters records: {len(pg_clusters_df)}\")\n",
        "print(f\"Final dataset after joining: {len(final_df)}\")\n",
        "print(f\"Database: {pg_params['database']}\")\n",
        "print(f\"Table: gist.gist_pageperformance\")\n",
        "print(f\"Process run ID: {run_id}\")\n",
        "print(f\"Process completed at: {datetime.now()}\")\n",
        "\n",
        "# Close connections\n",
        "mongo_client.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBDFIvBqvnkT",
        "outputId": "09f1ca92-9f30-4039-ad5c-e6925f306e95",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to PostgreSQL...\n",
            "Fetched 12962 clusters from PostgreSQL\n",
            "Normalizing post_link values...\n",
            "Connecting to MongoDB...\n",
            "Fetching MongoDB data with startDate >= 2024-11-19\n",
            "Fetched 596622 records from MongoDB (filtered for last 4 months)\n",
            "Normalizing page URLs...\n",
            "Joining MongoDB data with clusters using normalized URLs...\n",
            "After joining MongoDB with clusters: 323765 records\n",
            "Records lost in this join: 272857\n",
            "\n",
            "New matches found through normalization: 89232\n",
            "Sample of new matches (showing original URLs):\n",
            "                                                                         page                                                                         post_link\n",
            "0                       www.n90.asia/post/easy-business-capital-loans-options                     https://www.n90.asia/post/easy-business-capital-loans-options\n",
            "1    www.n90.asia/post/pros-and-cons-of-debt-consolidation-in-the-philippines  https://www.n90.asia/post/pros-and-cons-of-debt-consolidation-in-the-philippines\n",
            "3          https://auditive.io/blog/supplier-onboarding-checklist-development                 http://auditive.io/blog/supplier-onboarding-checklist-development\n",
            "5          https://auditive.io/blog/supplier-onboarding-checklist-development                 http://auditive.io/blog/supplier-onboarding-checklist-development\n",
            "172        https://superkalam.com/blog/ethics-notes-for-upsc-exam-preparation           https://blog.superkalam.com/blog/ethics-notes-for-upsc-exam-preparation\n",
            "\n",
            "WARNING: Found 18041 rows with duplicate composite keys\n",
            "Removing duplicate composite keys (keeping first occurrence only)\n",
            "Dataset after removing duplicates: 314356 records\n",
            "Final dataset prepared with 314356 records\n",
            "\n",
            "===== STEP 1: Setting up database schema =====\n",
            "Ensured schema 'gist' exists\n",
            "Table already exists - truncating it for fresh data\n",
            "Dropped dependent views\n",
            "Successfully truncated the table\n",
            "\n",
            "===== STEP 2: Creating table indexes =====\n",
            "Created index 1/5\n",
            "Created index 2/5\n",
            "Created index 3/5\n",
            "Created index 4/5\n",
            "Created index 5/5\n",
            "Created 5/5 indexes\n",
            "\n",
            "===== STEP 3: Inserting data =====\n",
            "Checking for duplicate composite keys in the dataset...\n",
            "Inserting 314356 rows in 158 batches (size 2000)\n",
            "Processing batch 1/158 (2000 records)\n",
            "Batch 1 processed, total progress: 2000/314356 rows\n",
            "Processing batch 2/158 (2000 records)\n",
            "Batch 2 processed, total progress: 4000/314356 rows\n",
            "Processing batch 3/158 (2000 records)\n",
            "Batch 3 processed, total progress: 6000/314356 rows\n",
            "Processing batch 4/158 (2000 records)\n",
            "Batch 4 processed, total progress: 8000/314356 rows\n",
            "Processing batch 5/158 (2000 records)\n",
            "Batch 5 processed, total progress: 10000/314356 rows\n",
            "Processing batch 6/158 (2000 records)\n",
            "Batch 6 processed, total progress: 12000/314356 rows\n",
            "Processing batch 7/158 (2000 records)\n",
            "Batch 7 processed, total progress: 14000/314356 rows\n",
            "Processing batch 8/158 (2000 records)\n",
            "Batch 8 processed, total progress: 16000/314356 rows\n",
            "Processing batch 9/158 (2000 records)\n",
            "Batch 9 processed, total progress: 18000/314356 rows\n",
            "Processing batch 10/158 (2000 records)\n",
            "Batch 10 processed, total progress: 20000/314356 rows\n",
            "Processing batch 11/158 (2000 records)\n",
            "Batch 11 processed, total progress: 22000/314356 rows\n",
            "Processing batch 12/158 (2000 records)\n",
            "Batch 12 processed, total progress: 24000/314356 rows\n",
            "Processing batch 13/158 (2000 records)\n",
            "Batch 13 processed, total progress: 26000/314356 rows\n",
            "Processing batch 14/158 (2000 records)\n",
            "Batch 14 processed, total progress: 28000/314356 rows\n",
            "Processing batch 15/158 (2000 records)\n",
            "Batch 15 processed, total progress: 30000/314356 rows\n",
            "Processing batch 16/158 (2000 records)\n",
            "Batch 16 processed, total progress: 32000/314356 rows\n",
            "Processing batch 17/158 (2000 records)\n",
            "Batch 17 processed, total progress: 34000/314356 rows\n",
            "Processing batch 18/158 (2000 records)\n",
            "Batch 18 processed, total progress: 36000/314356 rows\n",
            "Processing batch 19/158 (2000 records)\n",
            "Batch 19 processed, total progress: 38000/314356 rows\n",
            "Processing batch 20/158 (2000 records)\n",
            "Batch 20 processed, total progress: 40000/314356 rows\n",
            "Processing batch 21/158 (2000 records)\n",
            "Batch 21 processed, total progress: 42000/314356 rows\n",
            "Processing batch 22/158 (2000 records)\n",
            "Batch 22 processed, total progress: 44000/314356 rows\n",
            "Processing batch 23/158 (2000 records)\n",
            "Batch 23 processed, total progress: 46000/314356 rows\n",
            "Processing batch 24/158 (2000 records)\n",
            "Batch 24 processed, total progress: 48000/314356 rows\n",
            "Processing batch 25/158 (2000 records)\n",
            "Batch 25 processed, total progress: 50000/314356 rows\n",
            "Processing batch 26/158 (2000 records)\n",
            "Batch 26 processed, total progress: 52000/314356 rows\n",
            "Processing batch 27/158 (2000 records)\n",
            "Batch 27 processed, total progress: 54000/314356 rows\n",
            "Processing batch 28/158 (2000 records)\n",
            "Batch 28 processed, total progress: 56000/314356 rows\n",
            "Processing batch 29/158 (2000 records)\n",
            "Batch 29 processed, total progress: 58000/314356 rows\n",
            "Processing batch 30/158 (2000 records)\n",
            "Batch 30 processed, total progress: 60000/314356 rows\n",
            "Processing batch 31/158 (2000 records)\n",
            "Batch 31 processed, total progress: 62000/314356 rows\n",
            "Processing batch 32/158 (2000 records)\n",
            "Batch 32 processed, total progress: 64000/314356 rows\n",
            "Processing batch 33/158 (2000 records)\n",
            "Batch 33 processed, total progress: 66000/314356 rows\n",
            "Processing batch 34/158 (2000 records)\n",
            "Batch 34 processed, total progress: 68000/314356 rows\n",
            "Processing batch 35/158 (2000 records)\n",
            "Batch 35 processed, total progress: 70000/314356 rows\n",
            "Processing batch 36/158 (2000 records)\n",
            "Batch 36 processed, total progress: 72000/314356 rows\n",
            "Processing batch 37/158 (2000 records)\n",
            "Batch 37 processed, total progress: 74000/314356 rows\n",
            "Processing batch 38/158 (2000 records)\n",
            "Batch 38 processed, total progress: 76000/314356 rows\n",
            "Processing batch 39/158 (2000 records)\n",
            "Batch 39 processed, total progress: 78000/314356 rows\n",
            "Processing batch 40/158 (2000 records)\n",
            "Batch 40 processed, total progress: 80000/314356 rows\n",
            "Processing batch 41/158 (2000 records)\n",
            "Batch 41 processed, total progress: 82000/314356 rows\n",
            "Processing batch 42/158 (2000 records)\n",
            "Batch 42 processed, total progress: 84000/314356 rows\n",
            "Processing batch 43/158 (2000 records)\n",
            "Batch 43 processed, total progress: 86000/314356 rows\n",
            "Processing batch 44/158 (2000 records)\n",
            "Batch 44 processed, total progress: 88000/314356 rows\n",
            "Processing batch 45/158 (2000 records)\n",
            "Batch 45 processed, total progress: 90000/314356 rows\n",
            "Processing batch 46/158 (2000 records)\n",
            "Batch 46 processed, total progress: 92000/314356 rows\n",
            "Processing batch 47/158 (2000 records)\n",
            "Batch 47 processed, total progress: 94000/314356 rows\n",
            "Processing batch 48/158 (2000 records)\n",
            "Batch 48 processed, total progress: 96000/314356 rows\n",
            "Processing batch 49/158 (2000 records)\n",
            "Batch 49 processed, total progress: 98000/314356 rows\n",
            "Processing batch 50/158 (2000 records)\n",
            "Batch 50 processed, total progress: 100000/314356 rows\n",
            "Processing batch 51/158 (2000 records)\n",
            "Batch 51 processed, total progress: 102000/314356 rows\n",
            "Processing batch 52/158 (2000 records)\n",
            "Batch 52 processed, total progress: 104000/314356 rows\n",
            "Processing batch 53/158 (2000 records)\n",
            "Batch 53 processed, total progress: 106000/314356 rows\n",
            "Processing batch 54/158 (2000 records)\n",
            "Batch 54 processed, total progress: 108000/314356 rows\n",
            "Processing batch 55/158 (2000 records)\n",
            "Batch 55 processed, total progress: 110000/314356 rows\n",
            "Processing batch 56/158 (2000 records)\n",
            "Batch 56 processed, total progress: 112000/314356 rows\n",
            "Processing batch 57/158 (2000 records)\n",
            "Batch 57 processed, total progress: 114000/314356 rows\n",
            "Processing batch 58/158 (2000 records)\n",
            "Batch 58 processed, total progress: 116000/314356 rows\n",
            "Processing batch 59/158 (2000 records)\n",
            "Batch 59 processed, total progress: 118000/314356 rows\n",
            "Processing batch 60/158 (2000 records)\n",
            "Batch 60 processed, total progress: 120000/314356 rows\n",
            "Processing batch 61/158 (2000 records)\n",
            "Batch 61 processed, total progress: 122000/314356 rows\n",
            "Processing batch 62/158 (2000 records)\n",
            "Batch 62 processed, total progress: 124000/314356 rows\n",
            "Processing batch 63/158 (2000 records)\n",
            "Batch 63 processed, total progress: 126000/314356 rows\n",
            "Processing batch 64/158 (2000 records)\n",
            "Batch 64 processed, total progress: 128000/314356 rows\n",
            "Processing batch 65/158 (2000 records)\n",
            "Batch 65 processed, total progress: 130000/314356 rows\n",
            "Processing batch 66/158 (2000 records)\n",
            "Batch 66 processed, total progress: 132000/314356 rows\n",
            "Processing batch 67/158 (2000 records)\n",
            "Batch 67 processed, total progress: 134000/314356 rows\n",
            "Processing batch 68/158 (2000 records)\n",
            "Batch 68 processed, total progress: 136000/314356 rows\n",
            "Processing batch 69/158 (2000 records)\n",
            "Batch 69 processed, total progress: 138000/314356 rows\n",
            "Processing batch 70/158 (2000 records)\n",
            "Batch 70 processed, total progress: 140000/314356 rows\n",
            "Processing batch 71/158 (2000 records)\n",
            "Batch 71 processed, total progress: 142000/314356 rows\n",
            "Processing batch 72/158 (2000 records)\n",
            "Batch 72 processed, total progress: 144000/314356 rows\n",
            "Processing batch 73/158 (2000 records)\n",
            "Batch 73 processed, total progress: 146000/314356 rows\n",
            "Processing batch 74/158 (2000 records)\n",
            "Batch 74 processed, total progress: 148000/314356 rows\n",
            "Processing batch 75/158 (2000 records)\n",
            "Batch 75 processed, total progress: 150000/314356 rows\n",
            "Processing batch 76/158 (2000 records)\n",
            "Batch 76 processed, total progress: 152000/314356 rows\n",
            "Processing batch 77/158 (2000 records)\n",
            "Batch 77 processed, total progress: 154000/314356 rows\n",
            "Processing batch 78/158 (2000 records)\n",
            "Batch 78 processed, total progress: 156000/314356 rows\n",
            "Processing batch 79/158 (2000 records)\n",
            "Batch 79 processed, total progress: 158000/314356 rows\n",
            "Processing batch 80/158 (2000 records)\n",
            "Batch 80 processed, total progress: 160000/314356 rows\n",
            "Processing batch 81/158 (2000 records)\n",
            "Batch 81 processed, total progress: 162000/314356 rows\n",
            "Processing batch 82/158 (2000 records)\n",
            "Batch 82 processed, total progress: 164000/314356 rows\n",
            "Processing batch 83/158 (2000 records)\n",
            "Batch 83 processed, total progress: 166000/314356 rows\n",
            "Processing batch 84/158 (2000 records)\n",
            "Batch 84 processed, total progress: 168000/314356 rows\n",
            "Processing batch 85/158 (2000 records)\n",
            "Batch 85 processed, total progress: 170000/314356 rows\n",
            "Processing batch 86/158 (2000 records)\n",
            "Batch 86 processed, total progress: 172000/314356 rows\n",
            "Processing batch 87/158 (2000 records)\n",
            "Batch 87 processed, total progress: 174000/314356 rows\n",
            "Processing batch 88/158 (2000 records)\n",
            "Batch 88 processed, total progress: 176000/314356 rows\n",
            "Processing batch 89/158 (2000 records)\n",
            "Batch 89 processed, total progress: 178000/314356 rows\n",
            "Processing batch 90/158 (2000 records)\n",
            "Batch 90 processed, total progress: 180000/314356 rows\n",
            "Processing batch 91/158 (2000 records)\n",
            "Batch 91 processed, total progress: 182000/314356 rows\n",
            "Processing batch 92/158 (2000 records)\n",
            "Batch 92 processed, total progress: 184000/314356 rows\n",
            "Processing batch 93/158 (2000 records)\n",
            "Batch 93 processed, total progress: 186000/314356 rows\n",
            "Processing batch 94/158 (2000 records)\n",
            "Batch 94 processed, total progress: 188000/314356 rows\n",
            "Processing batch 95/158 (2000 records)\n",
            "Batch 95 processed, total progress: 190000/314356 rows\n",
            "Processing batch 96/158 (2000 records)\n",
            "Batch 96 processed, total progress: 192000/314356 rows\n",
            "Processing batch 97/158 (2000 records)\n",
            "Batch 97 processed, total progress: 194000/314356 rows\n",
            "Processing batch 98/158 (2000 records)\n",
            "Batch 98 processed, total progress: 196000/314356 rows\n",
            "Processing batch 99/158 (2000 records)\n",
            "Batch 99 processed, total progress: 198000/314356 rows\n",
            "Processing batch 100/158 (2000 records)\n",
            "Batch 100 processed, total progress: 200000/314356 rows\n",
            "Processing batch 101/158 (2000 records)\n",
            "Batch 101 processed, total progress: 202000/314356 rows\n",
            "Processing batch 102/158 (2000 records)\n",
            "Batch 102 processed, total progress: 204000/314356 rows\n",
            "Processing batch 103/158 (2000 records)\n",
            "Batch 103 processed, total progress: 206000/314356 rows\n",
            "Processing batch 104/158 (2000 records)\n",
            "Batch 104 processed, total progress: 208000/314356 rows\n",
            "Processing batch 105/158 (2000 records)\n",
            "Batch 105 processed, total progress: 210000/314356 rows\n",
            "Processing batch 106/158 (2000 records)\n",
            "Batch 106 processed, total progress: 212000/314356 rows\n",
            "Processing batch 107/158 (2000 records)\n",
            "Batch 107 processed, total progress: 214000/314356 rows\n",
            "Processing batch 108/158 (2000 records)\n",
            "Batch 108 processed, total progress: 216000/314356 rows\n",
            "Processing batch 109/158 (2000 records)\n",
            "Batch 109 processed, total progress: 218000/314356 rows\n",
            "Processing batch 110/158 (2000 records)\n",
            "Batch 110 processed, total progress: 220000/314356 rows\n",
            "Processing batch 111/158 (2000 records)\n",
            "Batch 111 processed, total progress: 222000/314356 rows\n",
            "Processing batch 112/158 (2000 records)\n",
            "Batch 112 processed, total progress: 224000/314356 rows\n",
            "Processing batch 113/158 (2000 records)\n",
            "Batch 113 processed, total progress: 226000/314356 rows\n",
            "Processing batch 114/158 (2000 records)\n",
            "Batch 114 processed, total progress: 228000/314356 rows\n",
            "Processing batch 115/158 (2000 records)\n",
            "Batch 115 processed, total progress: 230000/314356 rows\n",
            "Processing batch 116/158 (2000 records)\n",
            "Batch 116 processed, total progress: 232000/314356 rows\n",
            "Processing batch 117/158 (2000 records)\n",
            "Batch 117 processed, total progress: 234000/314356 rows\n",
            "Processing batch 118/158 (2000 records)\n",
            "Batch 118 processed, total progress: 236000/314356 rows\n",
            "Processing batch 119/158 (2000 records)\n",
            "Batch 119 processed, total progress: 238000/314356 rows\n",
            "Processing batch 120/158 (2000 records)\n",
            "Batch 120 processed, total progress: 240000/314356 rows\n",
            "Processing batch 121/158 (2000 records)\n",
            "Batch 121 processed, total progress: 242000/314356 rows\n",
            "Processing batch 122/158 (2000 records)\n",
            "Batch 122 processed, total progress: 244000/314356 rows\n",
            "Processing batch 123/158 (2000 records)\n",
            "Batch 123 processed, total progress: 246000/314356 rows\n",
            "Processing batch 124/158 (2000 records)\n",
            "Batch 124 processed, total progress: 248000/314356 rows\n",
            "Processing batch 125/158 (2000 records)\n",
            "Batch 125 processed, total progress: 250000/314356 rows\n",
            "Processing batch 126/158 (2000 records)\n",
            "Batch 126 processed, total progress: 252000/314356 rows\n",
            "Processing batch 127/158 (2000 records)\n",
            "Batch 127 processed, total progress: 254000/314356 rows\n",
            "Processing batch 128/158 (2000 records)\n",
            "Batch 128 processed, total progress: 256000/314356 rows\n",
            "Processing batch 129/158 (2000 records)\n",
            "Batch 129 processed, total progress: 258000/314356 rows\n",
            "Processing batch 130/158 (2000 records)\n",
            "Batch 130 processed, total progress: 260000/314356 rows\n",
            "Processing batch 131/158 (2000 records)\n",
            "Batch 131 processed, total progress: 262000/314356 rows\n",
            "Processing batch 132/158 (2000 records)\n",
            "Batch 132 processed, total progress: 264000/314356 rows\n",
            "Processing batch 133/158 (2000 records)\n",
            "Batch 133 processed, total progress: 266000/314356 rows\n",
            "Processing batch 134/158 (2000 records)\n",
            "Batch 134 processed, total progress: 268000/314356 rows\n",
            "Processing batch 135/158 (2000 records)\n",
            "Batch 135 processed, total progress: 270000/314356 rows\n",
            "Processing batch 136/158 (2000 records)\n",
            "Batch 136 processed, total progress: 272000/314356 rows\n",
            "Processing batch 137/158 (2000 records)\n",
            "Batch 137 processed, total progress: 274000/314356 rows\n",
            "Processing batch 138/158 (2000 records)\n",
            "Batch 138 processed, total progress: 276000/314356 rows\n",
            "Processing batch 139/158 (2000 records)\n",
            "Batch 139 processed, total progress: 278000/314356 rows\n",
            "Processing batch 140/158 (2000 records)\n",
            "Batch 140 processed, total progress: 280000/314356 rows\n",
            "Processing batch 141/158 (2000 records)\n",
            "Batch 141 processed, total progress: 282000/314356 rows\n",
            "Processing batch 142/158 (2000 records)\n",
            "Batch 142 processed, total progress: 284000/314356 rows\n",
            "Processing batch 143/158 (2000 records)\n",
            "Batch 143 processed, total progress: 286000/314356 rows\n",
            "Processing batch 144/158 (2000 records)\n",
            "Batch 144 processed, total progress: 288000/314356 rows\n",
            "Processing batch 145/158 (2000 records)\n",
            "Batch 145 processed, total progress: 290000/314356 rows\n",
            "Processing batch 146/158 (2000 records)\n",
            "Batch 146 processed, total progress: 292000/314356 rows\n",
            "Processing batch 147/158 (2000 records)\n",
            "Batch 147 processed, total progress: 294000/314356 rows\n",
            "Processing batch 148/158 (2000 records)\n",
            "Batch 148 processed, total progress: 296000/314356 rows\n",
            "Processing batch 149/158 (2000 records)\n",
            "Batch 149 processed, total progress: 298000/314356 rows\n",
            "Processing batch 150/158 (2000 records)\n",
            "Batch 150 processed, total progress: 300000/314356 rows\n",
            "Processing batch 151/158 (2000 records)\n",
            "Batch 151 processed, total progress: 302000/314356 rows\n",
            "Processing batch 152/158 (2000 records)\n",
            "Batch 152 processed, total progress: 304000/314356 rows\n",
            "Processing batch 153/158 (2000 records)\n",
            "Batch 153 processed, total progress: 306000/314356 rows\n",
            "Processing batch 154/158 (2000 records)\n",
            "Batch 154 processed, total progress: 308000/314356 rows\n",
            "Processing batch 155/158 (2000 records)\n",
            "Batch 155 processed, total progress: 310000/314356 rows\n",
            "Processing batch 156/158 (2000 records)\n",
            "Batch 156 processed, total progress: 312000/314356 rows\n",
            "Processing batch 157/158 (2000 records)\n",
            "Batch 157 processed, total progress: 314000/314356 rows\n",
            "Processing batch 158/158 (356 records)\n",
            "Batch 158 processed, total progress: 314356/314356 rows\n",
            "Final connection verified and committed\n",
            "\n",
            "===== STEP 4: Validating inserted data =====\n",
            "Records in database: 314356\n",
            "Expected records: 314356\n",
            "Sample records from database:\n",
            "  - ('6e7b5beb-9ad6-46f7-9c37-a87879172ab3_www.n90.asia/post/easy-business-capital-loans-options_20241121', '6e7b5beb-9ad6-46f7-9c37-a87879172ab3', 'www.n90.asia/post/easy-business-capital-loans-options', 'https://www.n90.asia/post/easy-business-capital-loans-options')\n",
            "  - ('8fc26d3f-bd85-4957-9d5d-d6dba971d267_www.n90.asia/post/pros-and-cons-of-debt-consolidation-in-the-philippines_20241121', '8fc26d3f-bd85-4957-9d5d-d6dba971d267', 'www.n90.asia/post/pros-and-cons-of-debt-consolidation-in-the-philippines', 'https://www.n90.asia/post/pros-and-cons-of-debt-consolidation-in-the-philippines')\n",
            "  - ('b6e947f0-a7df-445c-bbc4-5a219920f5d6_https://auditive.io/blog/supplier-onboarding-checklist-development_20241128', 'b6e947f0-a7df-445c-bbc4-5a219920f5d6', 'https://auditive.io/blog/supplier-onboarding-checklist-development', 'https://auditive.io/blog/supplier-onboarding-checklist-development')\n",
            "  - ('a7d1385a-fc65-4168-bb6f-16dbdb3c30b3_https://auditive.io/blog/supplier-onboarding-checklist-development_20241128', 'a7d1385a-fc65-4168-bb6f-16dbdb3c30b3', 'https://auditive.io/blog/supplier-onboarding-checklist-development', 'http://auditive.io/blog/supplier-onboarding-checklist-development')\n",
            "  - ('b6e947f0-a7df-445c-bbc4-5a219920f5d6_https://auditive.io/blog/supplier-onboarding-checklist-development_20241121', 'b6e947f0-a7df-445c-bbc4-5a219920f5d6', 'https://auditive.io/blog/supplier-onboarding-checklist-development', 'https://auditive.io/blog/supplier-onboarding-checklist-development')\n",
            "\n",
            "Data statistics:\n",
            "Total records: 314356\n",
            "Distinct writers: 205\n",
            "Distinct domains: 196\n",
            "Date range: 2024-11-21 00:00:00+00:00 to 2025-05-15 00:00:00+00:00\n",
            "\n",
            "Detailed count verification:\n",
            "COUNT(*): 314356\n",
            "COUNT(1): 314356\n",
            "COUNT(DISTINCT composite_key): 314356\n",
            "COUNT(DISTINCT page): 14192\n",
            "Records from current run (process_run_id = 20250617082126): 314356\n",
            "\n",
            "===== FINAL VERIFICATION =====\n",
            "Final count in database: 314356\n",
            "Expected count: 314356\n",
            "Records with current run_id (20250617082126): 314356\n",
            "\n",
            "===== SUMMARY =====\n",
            "MongoDB records (filtered for last 4 months): 596622\n",
            "Clusters records: 12962\n",
            "Final dataset after joining: 314356\n",
            "Database: gw_prod\n",
            "Table: gist.gist_pageperformance\n",
            "Process run ID: 20250617082126\n",
            "Process completed at: 2025-06-17 08:27:10.408583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "97tQQ-V7uT13"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}