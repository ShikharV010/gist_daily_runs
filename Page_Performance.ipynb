{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShikharV010/gist_daily_runs/blob/main/Page_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6JvkR2Zs7tj",
        "outputId": "84e743a7-123a-493f-c7fd-6fe028ef72e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.12/dist-packages (2.0.43)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading psycopg2_binary-2.9.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.9.10\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading pymongo-4.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.8.0 pymongo-4.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install psycopg2-binary sqlalchemy pandas\n",
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBDFIvBqvnkT",
        "outputId": "ffad9ec6-13d9-4e68-e135-6e17652a2bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to PostgreSQL...\n",
            "Fetched 29366 clusters from PostgreSQL\n",
            "Normalizing post_link values...\n",
            "Connecting to MongoDB...\n",
            "Fetching all documents from MongoDB (no date filter)...\n",
            "Fetched 1785129 records from MongoDB (filtered for last 4 months)\n",
            "Normalizing page URLs...\n",
            "Joining MongoDB data with clusters using normalized URLs...\n",
            "After joining MongoDB with clusters: 1813834 records\n",
            "Records lost in this join: -28705\n",
            "\n",
            "New matches found through normalization: 667933\n",
            "Sample of new matches (showing original URLs):\n",
            "                                                                                                    page                                                                                      post_link\n",
            "0            https://www.thesoundartist.com/blog/guitar-lessons-in-nassau-county-find-your-perfect-match            www.thesoundartist.com/blog/guitar-lessons-in-nassau-county-find-your-perfect-match\n",
            "1  https://www.thesoundartist.com/blog/handpan-lessons-in-huntington-with-monthly-sound-artist-workshops  www.thesoundartist.com/blog/handpan-lessons-in-huntington-with-monthly-sound-artist-workshops\n",
            "2            https://www.thesoundartist.com/blog/guitar-lessons-in-nassau-county-find-your-perfect-match            www.thesoundartist.com/blog/guitar-lessons-in-nassau-county-find-your-perfect-match\n",
            "3                                        https://www.thesoundartist.com/blog/handpan-lesson-northport-ny                                        www.thesoundartist.com/blog/handpan-lesson-northport-ny\n",
            "4  https://www.thesoundartist.com/blog/handpan-lessons-in-huntington-with-monthly-sound-artist-workshops  www.thesoundartist.com/blog/handpan-lessons-in-huntington-with-monthly-sound-artist-workshops\n",
            "\n",
            "WARNING: Found 127077 rows with duplicate composite keys\n",
            "Removing duplicate composite keys (keeping first occurrence only)\n",
            "Dataset after removing duplicates: 1748020 records\n",
            "Final dataset prepared with 1748020 records\n",
            "\n",
            "===== STEP 1: Setting up database schema =====\n",
            "Ensured schema 'gist' exists\n",
            "Table already exists - truncating it for fresh data\n",
            "Dropped dependent views\n",
            "Successfully truncated the table\n",
            "\n",
            "===== STEP 2: Creating table indexes =====\n",
            "Created index 1/5\n",
            "Created index 2/5\n",
            "Created index 3/5\n",
            "Created index 4/5\n",
            "Created index 5/5\n",
            "Created 5/5 indexes\n",
            "\n",
            "===== STEP 3: Inserting data =====\n",
            "Checking for duplicate composite keys in the dataset...\n",
            "Inserting 1748020 rows in 875 batches (size 2000)\n",
            "Processing batch 1/875 (2000 records)\n",
            "Batch 1 processed, total progress: 2000/1748020 rows\n",
            "Processing batch 2/875 (2000 records)\n",
            "Batch 2 processed, total progress: 4000/1748020 rows\n",
            "Processing batch 3/875 (2000 records)\n",
            "Batch 3 processed, total progress: 6000/1748020 rows\n",
            "Processing batch 4/875 (2000 records)\n",
            "Batch 4 processed, total progress: 8000/1748020 rows\n",
            "Processing batch 5/875 (2000 records)\n",
            "Batch 5 processed, total progress: 10000/1748020 rows\n",
            "Processing batch 6/875 (2000 records)\n",
            "Batch 6 processed, total progress: 12000/1748020 rows\n",
            "Processing batch 7/875 (2000 records)\n",
            "Batch 7 processed, total progress: 14000/1748020 rows\n",
            "Processing batch 8/875 (2000 records)\n",
            "Batch 8 processed, total progress: 16000/1748020 rows\n",
            "Processing batch 9/875 (2000 records)\n",
            "Batch 9 processed, total progress: 18000/1748020 rows\n",
            "Processing batch 10/875 (2000 records)\n",
            "Batch 10 processed, total progress: 20000/1748020 rows\n",
            "Processing batch 11/875 (2000 records)\n",
            "Batch 11 processed, total progress: 22000/1748020 rows\n",
            "Processing batch 12/875 (2000 records)\n",
            "Batch 12 processed, total progress: 24000/1748020 rows\n",
            "Processing batch 13/875 (2000 records)\n",
            "Batch 13 processed, total progress: 26000/1748020 rows\n",
            "Processing batch 14/875 (2000 records)\n",
            "Batch 14 processed, total progress: 28000/1748020 rows\n",
            "Processing batch 15/875 (2000 records)\n",
            "Batch 15 processed, total progress: 30000/1748020 rows\n",
            "Processing batch 16/875 (2000 records)\n",
            "Batch 16 processed, total progress: 32000/1748020 rows\n",
            "Processing batch 17/875 (2000 records)\n",
            "Batch 17 processed, total progress: 34000/1748020 rows\n",
            "Processing batch 18/875 (2000 records)\n",
            "Batch 18 processed, total progress: 36000/1748020 rows\n",
            "Processing batch 19/875 (2000 records)\n",
            "Batch 19 processed, total progress: 38000/1748020 rows\n",
            "Processing batch 20/875 (2000 records)\n",
            "Batch 20 processed, total progress: 40000/1748020 rows\n",
            "Processing batch 21/875 (2000 records)\n",
            "Batch 21 processed, total progress: 42000/1748020 rows\n",
            "Processing batch 22/875 (2000 records)\n",
            "Batch 22 processed, total progress: 44000/1748020 rows\n",
            "Processing batch 23/875 (2000 records)\n",
            "Batch 23 processed, total progress: 46000/1748020 rows\n",
            "Processing batch 24/875 (2000 records)\n",
            "Batch 24 processed, total progress: 48000/1748020 rows\n",
            "Processing batch 25/875 (2000 records)\n",
            "Batch 25 processed, total progress: 50000/1748020 rows\n",
            "Processing batch 26/875 (2000 records)\n",
            "Batch 26 processed, total progress: 52000/1748020 rows\n",
            "Processing batch 27/875 (2000 records)\n",
            "Batch 27 processed, total progress: 54000/1748020 rows\n",
            "Processing batch 28/875 (2000 records)\n",
            "Batch 28 processed, total progress: 56000/1748020 rows\n",
            "Processing batch 29/875 (2000 records)\n",
            "Batch 29 processed, total progress: 58000/1748020 rows\n",
            "Processing batch 30/875 (2000 records)\n",
            "Batch 30 processed, total progress: 60000/1748020 rows\n",
            "Processing batch 31/875 (2000 records)\n",
            "Batch 31 processed, total progress: 62000/1748020 rows\n",
            "Processing batch 32/875 (2000 records)\n",
            "Batch 32 processed, total progress: 64000/1748020 rows\n",
            "Processing batch 33/875 (2000 records)\n",
            "Batch 33 processed, total progress: 66000/1748020 rows\n",
            "Processing batch 34/875 (2000 records)\n",
            "Batch 34 processed, total progress: 68000/1748020 rows\n",
            "Processing batch 35/875 (2000 records)\n",
            "Batch 35 processed, total progress: 70000/1748020 rows\n",
            "Processing batch 36/875 (2000 records)\n",
            "Batch 36 processed, total progress: 72000/1748020 rows\n",
            "Processing batch 37/875 (2000 records)\n",
            "Batch 37 processed, total progress: 74000/1748020 rows\n",
            "Processing batch 38/875 (2000 records)\n",
            "Batch 38 processed, total progress: 76000/1748020 rows\n",
            "Processing batch 39/875 (2000 records)\n",
            "Batch 39 processed, total progress: 78000/1748020 rows\n",
            "Processing batch 40/875 (2000 records)\n",
            "Batch 40 processed, total progress: 80000/1748020 rows\n",
            "Processing batch 41/875 (2000 records)\n",
            "Batch 41 processed, total progress: 82000/1748020 rows\n",
            "Processing batch 42/875 (2000 records)\n",
            "Batch 42 processed, total progress: 84000/1748020 rows\n",
            "Processing batch 43/875 (2000 records)\n",
            "Batch 43 processed, total progress: 86000/1748020 rows\n",
            "Processing batch 44/875 (2000 records)\n",
            "Batch 44 processed, total progress: 88000/1748020 rows\n",
            "Processing batch 45/875 (2000 records)\n",
            "Batch 45 processed, total progress: 90000/1748020 rows\n",
            "Processing batch 46/875 (2000 records)\n",
            "Batch 46 processed, total progress: 92000/1748020 rows\n",
            "Processing batch 47/875 (2000 records)\n",
            "Batch 47 processed, total progress: 94000/1748020 rows\n",
            "Processing batch 48/875 (2000 records)\n",
            "Batch 48 processed, total progress: 96000/1748020 rows\n",
            "Processing batch 49/875 (2000 records)\n",
            "Batch 49 processed, total progress: 98000/1748020 rows\n",
            "Processing batch 50/875 (2000 records)\n",
            "Batch 50 processed, total progress: 100000/1748020 rows\n",
            "Processing batch 51/875 (2000 records)\n",
            "Batch 51 processed, total progress: 102000/1748020 rows\n",
            "Processing batch 52/875 (2000 records)\n",
            "Batch 52 processed, total progress: 104000/1748020 rows\n",
            "Processing batch 53/875 (2000 records)\n",
            "Batch 53 processed, total progress: 106000/1748020 rows\n",
            "Processing batch 54/875 (2000 records)\n",
            "Batch 54 processed, total progress: 108000/1748020 rows\n",
            "Processing batch 55/875 (2000 records)\n",
            "Batch 55 processed, total progress: 110000/1748020 rows\n",
            "Processing batch 56/875 (2000 records)\n",
            "Batch 56 processed, total progress: 112000/1748020 rows\n",
            "Processing batch 57/875 (2000 records)\n",
            "Batch 57 processed, total progress: 114000/1748020 rows\n",
            "Processing batch 58/875 (2000 records)\n",
            "Batch 58 processed, total progress: 116000/1748020 rows\n",
            "Processing batch 59/875 (2000 records)\n",
            "Batch 59 processed, total progress: 118000/1748020 rows\n",
            "Processing batch 60/875 (2000 records)\n",
            "Batch 60 processed, total progress: 120000/1748020 rows\n",
            "Processing batch 61/875 (2000 records)\n",
            "Batch 61 processed, total progress: 122000/1748020 rows\n",
            "Processing batch 62/875 (2000 records)\n",
            "Batch 62 processed, total progress: 124000/1748020 rows\n",
            "Processing batch 63/875 (2000 records)\n",
            "Batch 63 processed, total progress: 126000/1748020 rows\n",
            "Processing batch 64/875 (2000 records)\n",
            "Batch 64 processed, total progress: 128000/1748020 rows\n",
            "Processing batch 65/875 (2000 records)\n",
            "Batch 65 processed, total progress: 130000/1748020 rows\n",
            "Processing batch 66/875 (2000 records)\n",
            "Batch 66 processed, total progress: 132000/1748020 rows\n",
            "Processing batch 67/875 (2000 records)\n",
            "Batch 67 processed, total progress: 134000/1748020 rows\n",
            "Processing batch 68/875 (2000 records)\n",
            "Batch 68 processed, total progress: 136000/1748020 rows\n",
            "Processing batch 69/875 (2000 records)\n",
            "Batch 69 processed, total progress: 138000/1748020 rows\n",
            "Processing batch 70/875 (2000 records)\n",
            "Batch 70 processed, total progress: 140000/1748020 rows\n",
            "Processing batch 71/875 (2000 records)\n",
            "Batch 71 processed, total progress: 142000/1748020 rows\n",
            "Processing batch 72/875 (2000 records)\n",
            "Batch 72 processed, total progress: 144000/1748020 rows\n",
            "Processing batch 73/875 (2000 records)\n",
            "Batch 73 processed, total progress: 146000/1748020 rows\n",
            "Processing batch 74/875 (2000 records)\n",
            "Batch 74 processed, total progress: 148000/1748020 rows\n",
            "Processing batch 75/875 (2000 records)\n",
            "Batch 75 processed, total progress: 150000/1748020 rows\n",
            "Processing batch 76/875 (2000 records)\n",
            "Batch 76 processed, total progress: 152000/1748020 rows\n",
            "Processing batch 77/875 (2000 records)\n",
            "Batch 77 processed, total progress: 154000/1748020 rows\n",
            "Processing batch 78/875 (2000 records)\n",
            "Batch 78 processed, total progress: 156000/1748020 rows\n",
            "Processing batch 79/875 (2000 records)\n",
            "Batch 79 processed, total progress: 158000/1748020 rows\n",
            "Processing batch 80/875 (2000 records)\n",
            "Batch 80 processed, total progress: 160000/1748020 rows\n",
            "Processing batch 81/875 (2000 records)\n",
            "Batch 81 processed, total progress: 162000/1748020 rows\n",
            "Processing batch 82/875 (2000 records)\n",
            "Batch 82 processed, total progress: 164000/1748020 rows\n",
            "Processing batch 83/875 (2000 records)\n",
            "Batch 83 processed, total progress: 166000/1748020 rows\n",
            "Processing batch 84/875 (2000 records)\n",
            "Batch 84 processed, total progress: 168000/1748020 rows\n",
            "Processing batch 85/875 (2000 records)\n",
            "Batch 85 processed, total progress: 170000/1748020 rows\n",
            "Processing batch 86/875 (2000 records)\n",
            "Batch 86 processed, total progress: 172000/1748020 rows\n",
            "Processing batch 87/875 (2000 records)\n",
            "Batch 87 processed, total progress: 174000/1748020 rows\n",
            "Processing batch 88/875 (2000 records)\n",
            "Batch 88 processed, total progress: 176000/1748020 rows\n",
            "Processing batch 89/875 (2000 records)\n",
            "Batch 89 processed, total progress: 178000/1748020 rows\n",
            "Processing batch 90/875 (2000 records)\n",
            "Batch 90 processed, total progress: 180000/1748020 rows\n",
            "Processing batch 91/875 (2000 records)\n",
            "Batch 91 processed, total progress: 182000/1748020 rows\n",
            "Processing batch 92/875 (2000 records)\n",
            "Batch 92 processed, total progress: 184000/1748020 rows\n",
            "Processing batch 93/875 (2000 records)\n",
            "Batch 93 processed, total progress: 186000/1748020 rows\n",
            "Processing batch 94/875 (2000 records)\n"
          ]
        }
      ],
      "source": [
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "import pandas as pd\n",
        "import re\n",
        "from sqlalchemy import create_engine, text\n",
        "import pytz\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "import time\n",
        "import math\n",
        "from urllib.parse import urlparse  # needed for improved normalize_url\n",
        "\n",
        "# MongoDB connection parameters\n",
        "mongo_connection_string = \"mongodb+srv://readonlyPAT:readonlyusermongopassword@serverlessinstance1-pe-0.e4u1z9c.mongodb.net/gw_seo_pat\"\n",
        "\n",
        "# PostgreSQL connection parameters\n",
        "pg_params = {\n",
        "    'host': 'gw-rds-prod.celzx4qnlkfp.us-east-1.rds.amazonaws.com',\n",
        "    'database': 'gw_prod',\n",
        "    'user': 'airbyte_user',\n",
        "    'password': 'airbyte_user_password',\n",
        "    'port': '5432'\n",
        "}\n",
        "\n",
        "# Function to clean domain names\n",
        "def clean_domain(domain):\n",
        "    if domain is None:\n",
        "        return None\n",
        "\n",
        "    # Remove 'sc-domain:' prefix\n",
        "    if 'sc-domain:' in domain:\n",
        "        domain = domain.replace('sc-domain:', '')\n",
        "\n",
        "    # Remove https://, http://, www., blog., blogs., trailing slashes, etc.\n",
        "    domain = re.sub(r'^https?://', '', domain)\n",
        "    domain = re.sub(r'^www\\.', '', domain)\n",
        "    domain = re.sub(r'^blog\\.', '', domain)\n",
        "    domain = re.sub(r'^blogs\\.', '', domain)\n",
        "    domain = domain.strip('/')\n",
        "\n",
        "    # Remove all TLDs (.com, .org, etc.) and any following content\n",
        "    domain = re.sub(r'\\.[a-zA-Z0-9]+(\\.[a-zA-Z0-9]+)*($|/.*$)', '', domain)\n",
        "\n",
        "    # Remove any remaining periods\n",
        "    domain = domain.replace('.', '')\n",
        "\n",
        "    return domain\n",
        "\n",
        "# Improved: normalize URLs using urlparse\n",
        "def normalize_url(url):\n",
        "    if not url:\n",
        "        return None\n",
        "\n",
        "    url = url.strip().lower()\n",
        "    # ensure urlparse sees a scheme\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        url = 'http://' + url\n",
        "\n",
        "    parsed = urlparse(url)\n",
        "    # remove www./blog./blogs. from the host\n",
        "    host = parsed.netloc.replace('www.', '').replace('blog.', '').replace('blogs.', '')\n",
        "    # rebuild as host + path, drop query/fragment, trim trailing slash\n",
        "    return (host + parsed.path).rstrip('/')\n",
        "\n",
        "\n",
        "# PostgreSQL optimization - create engine with pooling and timeout settings\n",
        "def create_optimized_engine(pg_params):\n",
        "    pg_conn_string = f\"postgresql://{pg_params['user']}:{pg_params['password']}@{pg_params['host']}:{pg_params['port']}/{pg_params['database']}\"\n",
        "    return create_engine(\n",
        "        pg_conn_string,\n",
        "        pool_size=10,              # Increased number of connections for larger batches\n",
        "        max_overflow=20,           # Allow more additional connections\n",
        "        pool_timeout=60,           # Longer wait for a connection\n",
        "        pool_recycle=1800,         # Recycle connections after 30 minutes\n",
        "        connect_args={\"options\": \"-c statement_timeout=600000\"}  # 10 minute default timeout\n",
        "    )\n",
        "\n",
        "# Create schema and table structure separately (avoid timeouts) - WITH TRUNCATE OPTION\n",
        "def setup_database_schema(engine):\n",
        "    schema_created = False\n",
        "    table_created = False\n",
        "\n",
        "    with engine.connect() as connection:\n",
        "        # Enable autocommit to ensure schema creation is persisted even if later operations fail\n",
        "        connection.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "\n",
        "        try:\n",
        "            # Try setting a very long timeout for schema operations\n",
        "            connection.execute(text(\"SET statement_timeout = 1200000;\"))  # 20 minutes\n",
        "\n",
        "            # Create schema if not exists\n",
        "            connection.execute(text(\"CREATE SCHEMA IF NOT EXISTS gist;\"))\n",
        "            schema_created = True\n",
        "            print(\"Ensured schema 'gist' exists\")\n",
        "\n",
        "            # Check if table exists\n",
        "            table_exists_query = \"\"\"\n",
        "            SELECT EXISTS (\n",
        "                SELECT 1 FROM information_schema.tables\n",
        "                WHERE table_schema = 'gist' AND table_name = 'gist_pageperformance'\n",
        "            );\n",
        "            \"\"\"\n",
        "            table_exists = connection.execute(text(table_exists_query)).scalar()\n",
        "\n",
        "            if not table_exists:\n",
        "                # Create table without indexes first for speed\n",
        "                create_table_sql = \"\"\"\n",
        "                CREATE TABLE gist.gist_pageperformance (\n",
        "                    composite_key TEXT PRIMARY KEY,\n",
        "                    cluster_id TEXT,\n",
        "                    writer TEXT,\n",
        "                    page TEXT,\n",
        "                    domain TEXT,\n",
        "                    cleaned_domain TEXT,\n",
        "                    normalized_page TEXT,\n",
        "                    normalized_post_link TEXT,\n",
        "                    start_date TIMESTAMP WITH TIME ZONE,\n",
        "                    end_date TIMESTAMP WITH TIME ZONE,\n",
        "                    clicks INTEGER,\n",
        "                    impressions INTEGER,\n",
        "                    ctr FLOAT,\n",
        "                    position FLOAT,\n",
        "                    post_link TEXT,\n",
        "                    delivery_date TIMESTAMP WITH TIME ZONE,\n",
        "                    process_run_id TEXT,\n",
        "                    processed_at TIMESTAMP WITH TIME ZONE\n",
        "                );\n",
        "                \"\"\"\n",
        "                connection.execute(text(create_table_sql))\n",
        "                table_created = True\n",
        "                print(\"Created table without indexes\")\n",
        "            else:\n",
        "                print(\"Table already exists - truncating it for fresh data\")\n",
        "\n",
        "                # First drop any views that might depend on this table\n",
        "                try:\n",
        "                    connection.execute(text(\"DROP VIEW IF EXISTS gist.writer_performance_view;\"))\n",
        "                    connection.execute(text(\"DROP VIEW IF EXISTS gist.domain_performance_view;\"))\n",
        "                    print(\"Dropped dependent views\")\n",
        "                except Exception as view_error:\n",
        "                    print(f\"Warning: Could not drop dependent views: {view_error}\")\n",
        "                    print(\"Proceeding with truncate anyway...\")\n",
        "\n",
        "                # Truncate the table (deletes all rows but keeps table structure and indexes)\n",
        "                truncate_sql = \"TRUNCATE TABLE gist.gist_pageperformance;\"\n",
        "                connection.execute(text(truncate_sql))\n",
        "                print(\"Successfully truncated the table\")\n",
        "                table_created = True  # Consider table ready\n",
        "\n",
        "            # Return true in both cases: new table or truncated existing table\n",
        "            return schema_created and table_created\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in schema/table setup: {e}\")\n",
        "            # Return partial success status\n",
        "            return schema_created and table_created\n",
        "\n",
        "# Create indexes separately to avoid timeout\n",
        "def create_table_indexes(engine):\n",
        "    with engine.connect() as connection:\n",
        "        # Enable autocommit for index operations\n",
        "        connection.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "\n",
        "        try:\n",
        "            # Set long timeout for index creation\n",
        "            connection.execute(text(\"SET statement_timeout = 600000;\"))  # 10 minutes\n",
        "\n",
        "            # Check if table exists\n",
        "            table_exists_query = \"\"\"\n",
        "            SELECT EXISTS (\n",
        "                SELECT 1 FROM information_schema.tables\n",
        "                WHERE table_schema = 'gist' AND table_name = 'gist_pageperformance'\n",
        "            );\n",
        "            \"\"\"\n",
        "            table_exists = connection.execute(text(table_exists_query)).scalar()\n",
        "\n",
        "            if not table_exists:\n",
        "                print(\"Table doesn't exist, cannot create indexes\")\n",
        "                return False\n",
        "\n",
        "            # Create each index separately with individual transactions\n",
        "            index_statements = [\n",
        "                \"\"\"CREATE INDEX IF NOT EXISTS idx_pageperformance_page\n",
        "                   ON gist.gist_pageperformance(page);\"\"\",\n",
        "                \"\"\"CREATE INDEX IF NOT EXISTS idx_pageperformance_domain\n",
        "                   ON gist.gist_pageperformance(cleaned_domain);\"\"\",\n",
        "                \"\"\"CREATE INDEX IF NOT EXISTS idx_pageperformance_start_date\n",
        "                   ON gist.gist_pageperformance(start_date);\"\"\",\n",
        "                \"\"\"CREATE INDEX IF NOT EXISTS idx_pageperformance_normalized_page\n",
        "                   ON gist.gist_pageperformance(normalized_page);\"\"\",\n",
        "                \"\"\"CREATE INDEX IF NOT EXISTS idx_pageperformance_normalized_post_link\n",
        "                   ON gist.gist_pageperformance(normalized_post_link);\"\"\"\n",
        "            ]\n",
        "\n",
        "            success_count = 0\n",
        "            for i, stmt in enumerate(index_statements):\n",
        "                try:\n",
        "                    # Create each index in a separate transaction\n",
        "                    with engine.connect() as idx_conn:\n",
        "                        idx_conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "                        idx_conn.execute(text(\"SET statement_timeout = 300000;\"))  # 5 minutes per index\n",
        "                        idx_conn.execute(text(stmt))\n",
        "                    success_count += 1\n",
        "                    print(f\"Created index {i+1}/{len(index_statements)}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not create index {i+1}: {e}\")\n",
        "                    print(\"Will continue without this index\")\n",
        "\n",
        "            print(f\"Created {success_count}/{len(index_statements)} indexes\")\n",
        "            return success_count > 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating indexes: {e}\")\n",
        "            return False\n",
        "\n",
        "# Direct batch insertion with larger batch size (1000 rows)\n",
        "def insert_data_in_batches(engine, dataframe, batch_size=50000):\n",
        "    if dataframe.empty:\n",
        "        print(\"No data to insert\")\n",
        "        return 0\n",
        "\n",
        "    # Remove duplicates before insertion to avoid primary key violations\n",
        "    print(\"Checking for duplicate composite keys in the dataset...\")\n",
        "    duplicates = dataframe.duplicated(subset=['composite_key'], keep='first')\n",
        "    if duplicates.any():\n",
        "        dup_count = duplicates.sum()\n",
        "        print(f\"Found {dup_count} duplicate composite keys. Keeping only the first occurrence of each.\")\n",
        "        # Keep only the first occurrence of each composite_key\n",
        "        dataframe = dataframe.drop_duplicates(subset=['composite_key'], keep='first')\n",
        "\n",
        "    total_rows = len(dataframe)\n",
        "    total_batches = math.ceil(total_rows / batch_size)\n",
        "    rows_inserted = 0\n",
        "\n",
        "    print(f\"Inserting {total_rows} rows in {total_batches} batches (size {batch_size})\")\n",
        "\n",
        "    for i in range(0, total_rows, batch_size):\n",
        "        batch_num = i // batch_size + 1\n",
        "        end_idx = min(i + batch_size, total_rows)\n",
        "        batch_df = dataframe.iloc[i:end_idx].copy()\n",
        "\n",
        "        print(f\"Processing batch {batch_num}/{total_batches} ({len(batch_df)} records)\")\n",
        "\n",
        "        try:\n",
        "            with engine.connect() as conn:\n",
        "                # Enable autocommit for this connection\n",
        "                conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "\n",
        "                # Set reasonable timeout\n",
        "                conn.execute(text(\"SET statement_timeout = 600000;\"))  # 10 minutes for larger batches\n",
        "\n",
        "                # Handle NULL values\n",
        "                batch_df = batch_df.where(pd.notnull(batch_df), None)\n",
        "\n",
        "                # Use to_sql with method='multi' for better performance\n",
        "                batch_df.to_sql(\n",
        "                    'gist_pageperformance',\n",
        "                    conn,\n",
        "                    schema='gist',\n",
        "                    if_exists='append',\n",
        "                    index=False,\n",
        "                    method='multi',\n",
        "                    chunksize=100  # Smaller internal chunks\n",
        "                )\n",
        "\n",
        "                rows_inserted += len(batch_df)\n",
        "                print(f\"Batch {batch_num} processed, total progress: {rows_inserted}/{total_rows} rows\")\n",
        "\n",
        "        except Exception as batch_error:\n",
        "            print(f\"Error processing batch {batch_num}: {str(batch_error)[:200]}...\")\n",
        "            print(\"Falling back to smaller batches for this chunk\")\n",
        "\n",
        "            # Try smaller batches as fallback\n",
        "            small_batch_size = 50\n",
        "            small_total_batches = math.ceil(len(batch_df) / small_batch_size)\n",
        "\n",
        "            for j in range(0, len(batch_df), small_batch_size):\n",
        "                small_batch_num = j // small_batch_size + 1\n",
        "                small_end_idx = min(j + small_batch_size, len(batch_df))\n",
        "                small_batch_df = batch_df.iloc[j:small_end_idx].copy()\n",
        "\n",
        "                print(f\"Processing small batch {small_batch_num}/{small_total_batches}\")\n",
        "\n",
        "                try:\n",
        "                    with engine.connect() as small_conn:\n",
        "                        small_conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "                        small_conn.execute(text(\"SET statement_timeout = 300000;\"))  # 5 minutes\n",
        "\n",
        "                        small_batch_df = small_batch_df.where(pd.notnull(small_batch_df), None)\n",
        "\n",
        "                        small_batch_df.to_sql(\n",
        "                            'gist_pageperformance',\n",
        "                            small_conn,\n",
        "                            schema='gist',\n",
        "                            if_exists='append',\n",
        "                            index=False,\n",
        "                            method='multi',\n",
        "                            chunksize=10\n",
        "                        )\n",
        "\n",
        "                        rows_inserted += len(small_batch_df)\n",
        "                        print(f\"Small batch processed, total progress: {rows_inserted}/{total_rows} rows\")\n",
        "\n",
        "                except Exception as small_batch_error:\n",
        "                    print(f\"Error processing small batch: {str(small_batch_error)[:200]}...\")\n",
        "                    print(\"Trying row-by-row for this batch\")\n",
        "\n",
        "                    # Try individual row insertion as final fallback\n",
        "                    for idx, row in small_batch_df.iterrows():\n",
        "                        try:\n",
        "                            # Handle NaN values\n",
        "                            row = row.where(pd.notnull(row), None)\n",
        "\n",
        "                            # Convert to DataFrame with single row\n",
        "                            single_row_df = pd.DataFrame([row.to_dict()])\n",
        "\n",
        "                            with engine.connect() as single_conn:\n",
        "                                single_conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "                                single_row_df.to_sql(\n",
        "                                    'gist_pageperformance',\n",
        "                                    single_conn,\n",
        "                                    schema='gist',\n",
        "                                    if_exists='append',\n",
        "                                    index=False\n",
        "                                )\n",
        "                                rows_inserted += 1\n",
        "\n",
        "                        except Exception as row_error:\n",
        "                            if \"duplicate key\" in str(row_error).lower():\n",
        "                                print(f\"Skipping duplicate key for row {idx}\")\n",
        "                            else:\n",
        "                                print(f\"Error inserting row {idx}: {str(row_error)[:100]}...\")\n",
        "\n",
        "        # Small pause between batches\n",
        "        time.sleep(0.3)\n",
        "\n",
        "    # Add an explicit final commit to ensure all data is committed\n",
        "    try:\n",
        "        with engine.connect() as final_conn:\n",
        "            final_conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "            # Execute a dummy query to ensure connection is good\n",
        "            final_conn.execute(text(\"SELECT 1;\"))\n",
        "            print(\"Final connection verified and committed\")\n",
        "    except Exception as e:\n",
        "        print(f\"Final commit attempt error: {e}\")\n",
        "\n",
        "    return rows_inserted\n",
        "\n",
        "# Step 4: Validate insertions and run consistency checks\n",
        "def validate_data(engine, original_df):\n",
        "    print(\"\\n===== STEP 4: Validating inserted data =====\")\n",
        "    try:\n",
        "        with engine.connect() as conn:\n",
        "            # Enable autocommit\n",
        "            conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "\n",
        "            # Set timeout\n",
        "            conn.execute(text(\"SET statement_timeout = 300000;\"))  # 5 minutes\n",
        "\n",
        "            # Check if the table exists\n",
        "            table_exists_query = \"\"\"\n",
        "            SELECT EXISTS (\n",
        "                SELECT 1 FROM information_schema.tables\n",
        "                WHERE table_schema = 'gist' AND table_name = 'gist_pageperformance'\n",
        "            );\n",
        "            \"\"\"\n",
        "            table_exists = conn.execute(text(table_exists_query)).scalar()\n",
        "\n",
        "            if not table_exists:\n",
        "                print(\"VALIDATION FAILED: Table does not exist!\")\n",
        "                return False\n",
        "\n",
        "            # Count records\n",
        "            count_query = \"SELECT COUNT(*) FROM gist.gist_pageperformance;\"\n",
        "            db_count = conn.execute(text(count_query)).scalar()\n",
        "\n",
        "            # Compare with expected count\n",
        "            expected_count = len(original_df.drop_duplicates(subset=['composite_key'], keep='first'))\n",
        "\n",
        "            print(f\"Records in database: {db_count}\")\n",
        "            print(f\"Expected records: {expected_count}\")\n",
        "\n",
        "            if db_count == 0:\n",
        "                print(\"VALIDATION FAILED: No records in database!\")\n",
        "                return False\n",
        "\n",
        "            # Sample some records to verify content\n",
        "            sample_query = \"\"\"\n",
        "            SELECT composite_key, cluster_id, page, post_link\n",
        "            FROM gist.gist_pageperformance\n",
        "            LIMIT 5;\n",
        "            \"\"\"\n",
        "\n",
        "            samples = conn.execute(text(sample_query)).fetchall()\n",
        "            if samples:\n",
        "                print(\"Sample records from database:\")\n",
        "                for sample in samples:\n",
        "                    print(f\"  - {sample}\")\n",
        "\n",
        "            # Check for data distribution\n",
        "            stats_query = \"\"\"\n",
        "            SELECT\n",
        "                COUNT(*) as total_records,\n",
        "                COUNT(DISTINCT writer) as distinct_writers,\n",
        "                COUNT(DISTINCT cleaned_domain) as distinct_domains,\n",
        "                MIN(start_date) as earliest_date,\n",
        "                MAX(start_date) as latest_date\n",
        "            FROM gist.gist_pageperformance;\n",
        "            \"\"\"\n",
        "\n",
        "            stats = conn.execute(text(stats_query)).fetchone()\n",
        "            if stats:\n",
        "                print(\"\\nData statistics:\")\n",
        "                print(f\"Total records: {stats[0]}\")\n",
        "                print(f\"Distinct writers: {stats[1]}\")\n",
        "                print(f\"Distinct domains: {stats[2]}\")\n",
        "                print(f\"Date range: {stats[3]} to {stats[4]}\")\n",
        "\n",
        "            # Additional count verification\n",
        "            try:\n",
        "                # Count with different methods\n",
        "                count1 = conn.execute(text(\"SELECT COUNT(*) FROM gist.gist_pageperformance;\")).scalar()\n",
        "                count2 = conn.execute(text(\"SELECT COUNT(1) FROM gist.gist_pageperformance;\")).scalar()\n",
        "\n",
        "                # Count by specific columns\n",
        "                count_by_key = conn.execute(text(\"SELECT COUNT(DISTINCT composite_key) FROM gist.gist_pageperformance;\")).scalar()\n",
        "                count_by_page = conn.execute(text(\"SELECT COUNT(DISTINCT page) FROM gist.gist_pageperformance;\")).scalar()\n",
        "\n",
        "                print(\"\\nDetailed count verification:\")\n",
        "                print(f\"COUNT(*): {count1}\")\n",
        "                print(f\"COUNT(1): {count2}\")\n",
        "                print(f\"COUNT(DISTINCT composite_key): {count_by_key}\")\n",
        "                print(f\"COUNT(DISTINCT page): {count_by_page}\")\n",
        "\n",
        "                # Check if all records from the current run are present\n",
        "                current_run_count = conn.execute(text(f\"SELECT COUNT(*) FROM gist.gist_pageperformance WHERE process_run_id = '{run_id}';\")).scalar()\n",
        "                print(f\"Records from current run (process_run_id = {run_id}): {current_run_count}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Additional verification failed: {e}\")\n",
        "\n",
        "            return db_count > 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Validation failed with error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Main script execution starts here\n",
        "print(\"Connecting to PostgreSQL...\")\n",
        "engine = create_optimized_engine(pg_params)\n",
        "\n",
        "# <<< CHANGED FILTER: only require a real post_link; status ignored >>>\n",
        "clusters_query = \"\"\"\n",
        "SELECT id, writer, post_link, post_date as delivery_date\n",
        "FROM public.clusters\n",
        "WHERE post_link IS NOT NULL\n",
        "  AND post_link <> ''\n",
        "\"\"\"\n",
        "pg_clusters_df = pd.read_sql(clusters_query, engine)\n",
        "print(f\"Fetched {len(pg_clusters_df)} clusters from PostgreSQL\")\n",
        "\n",
        "# Ensure dates are timezone-aware\n",
        "if 'delivery_date' in pg_clusters_df.columns:\n",
        "    pg_clusters_df['delivery_date'] = pd.to_datetime(pg_clusters_df['delivery_date'])\n",
        "    if pg_clusters_df['delivery_date'].dt.tz is None:\n",
        "        pg_clusters_df['delivery_date'] = pg_clusters_df['delivery_date'].dt.tz_localize('UTC')\n",
        "    else:\n",
        "        pg_clusters_df['delivery_date'] = pg_clusters_df['delivery_date'].dt.tz_convert('UTC')\n",
        "\n",
        "# Clean NA-ish post_link values before normalization (defensive)\n",
        "pg_clusters_df['post_link'] = (\n",
        "    pg_clusters_df['post_link']\n",
        "      .astype(str)\n",
        "      .str.strip()\n",
        "      .replace({'': None, 'nan': None, 'None': None}, regex=False)\n",
        ")\n",
        "\n",
        "# Apply normalization to post_link\n",
        "print(\"Normalizing post_link values...\")\n",
        "pg_clusters_df['normalized_post_link'] = pg_clusters_df['post_link'].apply(normalize_url)\n",
        "\n",
        "# Connect to MongoDB\n",
        "print(\"Connecting to MongoDB...\")\n",
        "mongo_client = MongoClient(mongo_connection_string)\n",
        "db_name = \"gw_seo_pat\"\n",
        "mongo_db = mongo_client[db_name]\n",
        "collection = mongo_db[\"page_performance\"]\n",
        "\n",
        "# Updated: Remove date filter to include all data\n",
        "query = {}\n",
        "print(\"Fetching all documents from MongoDB (no date filter)...\")\n",
        "\n",
        "# Get all documents\n",
        "all_docs = list(collection.find(query))\n",
        "\n",
        "for doc in all_docs:\n",
        "    if '_id' in doc:\n",
        "        doc['_id'] = str(doc['_id'])\n",
        "    if 'domain' in doc:\n",
        "        doc['cleaned_domain'] = clean_domain(doc['domain'])\n",
        "\n",
        "# Convert to DataFrame\n",
        "mongo_df = pd.DataFrame(all_docs)\n",
        "print(f\"Fetched {len(mongo_df)} records from MongoDB (filtered for last 4 months)\")  # message text left as-is per your original\n",
        "\n",
        "# Apply normalization to page URLs\n",
        "print(\"Normalizing page URLs...\")\n",
        "mongo_df['normalized_page'] = mongo_df['page'].apply(normalize_url)\n",
        "\n",
        "# Convert MongoDB date fields to pandas datetime with UTC timezone\n",
        "if 'startDate' in mongo_df.columns:\n",
        "    mongo_df['startDate'] = pd.to_datetime(mongo_df['startDate']).dt.tz_localize('UTC')\n",
        "if 'endDate' in mongo_df.columns:\n",
        "    mongo_df['endDate'] = pd.to_datetime(mongo_df['endDate']).dt.tz_localize('UTC')\n",
        "\n",
        "# Join MongoDB data with clusters based on normalized URLs\n",
        "print(\"Joining MongoDB data with clusters using normalized URLs...\")\n",
        "final_df = pd.merge(\n",
        "    mongo_df,\n",
        "    pg_clusters_df,\n",
        "    left_on='normalized_page',\n",
        "    right_on='normalized_post_link',\n",
        "    how='left'\n",
        ")\n",
        "print(f\"After joining MongoDB with clusters: {len(final_df)} records\")\n",
        "print(f\"Records lost in this join: {len(mongo_df) - len(final_df)}\")\n",
        "\n",
        "# Sample some new matches to validate normalization\n",
        "if len(final_df) > 0:\n",
        "    new_matches = final_df[final_df['page'] != final_df['post_link']]\n",
        "    if not new_matches.empty:\n",
        "        print(f\"\\nNew matches found through normalization: {len(new_matches)}\")\n",
        "        print(\"Sample of new matches (showing original URLs):\")\n",
        "        print(new_matches[['page', 'post_link']].head(5).to_string())\n",
        "\n",
        "# Generate run ID and timestamp for tracking\n",
        "run_id = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "current_timestamp = datetime.now()\n",
        "\n",
        "# Add processing metadata\n",
        "if not final_df.empty:\n",
        "    final_df['process_run_id'] = run_id\n",
        "    final_df['processed_at'] = current_timestamp\n",
        "\n",
        "    # Create a composite key for identifying records\n",
        "    final_df['composite_key'] = final_df.apply(\n",
        "        lambda row: f\"{row.get('id', '')}_{row.get('page', '')}_{row.get('startDate', '').strftime('%Y%m%d')}\",\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: Check for and handle duplicate composite keys before proceeding\n",
        "    duplicate_keys = final_df.duplicated(subset=['composite_key'], keep=False)\n",
        "    if duplicate_keys.any():\n",
        "        print(f\"\\nWARNING: Found {duplicate_keys.sum()} rows with duplicate composite keys\")\n",
        "        # Keep only the first occurrence of each composite_key\n",
        "        print(\"Removing duplicate composite keys (keeping first occurrence only)\")\n",
        "        final_df = final_df.drop_duplicates(subset=['composite_key'], keep='first')\n",
        "        print(f\"Dataset after removing duplicates: {len(final_df)} records\")\n",
        "\n",
        "    print(f\"Final dataset prepared with {len(final_df)} records\")\n",
        "\n",
        "    # Column mapping for database\n",
        "    column_mapping = {\n",
        "        'composite_key': 'composite_key',\n",
        "        'id': 'cluster_id',\n",
        "        'writer': 'writer',\n",
        "        'page': 'page',\n",
        "        'domain': 'domain',\n",
        "        'cleaned_domain': 'cleaned_domain',\n",
        "        'normalized_page': 'normalized_page',\n",
        "        'normalized_post_link': 'normalized_post_link',\n",
        "        'startDate': 'start_date',\n",
        "        'endDate': 'end_date',\n",
        "        'clicks': 'clicks',\n",
        "        'impressions': 'impressions',\n",
        "        'ctr': 'ctr',\n",
        "        'position': 'position',\n",
        "        'post_link': 'post_link',\n",
        "        'delivery_date': 'delivery_date',\n",
        "        'process_run_id': 'process_run_id',\n",
        "        'processed_at': 'processed_at'\n",
        "    }\n",
        "\n",
        "    # Only keep the columns we want\n",
        "    columns_to_keep = list(column_mapping.keys())\n",
        "    columns_to_keep = [col for col in columns_to_keep if col in final_df.columns]\n",
        "    subset_df = final_df[columns_to_keep].copy()\n",
        "\n",
        "    # Rename columns according to the mapping\n",
        "    renamed_df = subset_df.rename(columns={k: v for k, v in column_mapping.items() if k in subset_df.columns})\n",
        "\n",
        "    # Save original dataframe for validation\n",
        "    original_df = renamed_df.copy()\n",
        "\n",
        "    # STEP 1: Setup schema and table structure (separate step)\n",
        "    print(\"\\n===== STEP 1: Setting up database schema =====\")\n",
        "    setup_success = setup_database_schema(engine)\n",
        "\n",
        "    if setup_success:\n",
        "        # STEP 2: Create database indexes (separate step)\n",
        "        print(\"\\n===== STEP 2: Creating table indexes =====\")\n",
        "        index_success = create_table_indexes(engine)\n",
        "\n",
        "        # STEP 3: Insert data using direct batch insertion with duplicate handling\n",
        "        print(\"\\n===== STEP 3: Inserting data =====\")\n",
        "        # Use larger batch size (2000 rows)\n",
        "        rows_inserted = insert_data_in_batches(engine, renamed_df, batch_size=2000)\n",
        "\n",
        "        # STEP 4: Validate data insertion\n",
        "        validation_success = validate_data(engine, original_df)\n",
        "\n",
        "        # No dependent views creation - removed as requested\n",
        "    else:\n",
        "        print(\"Failed to set up database schema\")\n",
        "\n",
        "else:\n",
        "    print(\"No data to insert into PostgreSQL\")\n",
        "\n",
        "# Final verification\n",
        "print(\"\\n===== FINAL VERIFICATION =====\")\n",
        "try:\n",
        "    with engine.connect() as final_verify:\n",
        "        final_verify.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
        "        final_count = final_verify.execute(text(\"SELECT COUNT(*) FROM gist.gist_pageperformance;\")).scalar()\n",
        "        print(f\"Final count in database: {final_count}\")\n",
        "        print(f\"Expected count: {len(final_df.drop_duplicates(subset=['composite_key'], keep='first'))}\")\n",
        "\n",
        "        # Check for this run's records\n",
        "        run_count = final_verify.execute(text(f\"SELECT COUNT(*) FROM gist.gist_pageperformance WHERE process_run_id = '{run_id}';\")).scalar()\n",
        "        print(f\"Records with current run_id ({run_id}): {run_count}\")\n",
        "\n",
        "        # Check if there are older records\n",
        "        other_runs = final_verify.execute(text(f\"SELECT process_run_id, COUNT(*) FROM gist.gist_pageperformance WHERE process_run_id != '{run_id}' GROUP BY process_run_id;\")).fetchall()\n",
        "        if other_runs:\n",
        "            print(\"Found records from other runs:\")\n",
        "            for other_run in other_runs:\n",
        "                print(f\"  - Run ID {other_run[0]}: {other_run[1]} records\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Final verification failed: {e}\")\n",
        "\n",
        "# Output summary\n",
        "print(\"\\n===== SUMMARY =====\")\n",
        "print(f\"MongoDB records (filtered for last 4 months): {len(mongo_df)}\")\n",
        "print(f\"Clusters records: {len(pg_clusters_df)}\")\n",
        "print(f\"Final dataset after joining: {len(final_df)}\")\n",
        "print(f\"Database: {pg_params['database']}\")\n",
        "print(f\"Table: gist.gist_pageperformance\")\n",
        "print(f\"Process run ID: {run_id}\")\n",
        "print(f\"Process completed at: {datetime.now()}\")\n",
        "\n",
        "# Close connections\n",
        "mongo_client.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vmBd8XjcOKU"
      },
      "source": [
        "TESTING -"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}